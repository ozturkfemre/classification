library(caret)
library(tidyverse)
library(magrittr)
library(olsrr)
library(car)
library(corrplot)
library(ISLR)
library(Hmisc)
library(caret)
library(dplyr)
library(ModelMetrics)
library(lmtest)
library(moments)
library(bestNormalize) # normalization 
library(MASS)
library(psych) 
library(mvnTest) # perform multivariate normality test
library(tree) # perform regression and decision tree
library(randomForest) # perform random forest
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
library(kmed)
library(klaR)
library(e1071)
library(gridExtra)
library(ggalt)
#install.packages("ROCR")
library(ROCR)
library(MVN)

df <- get(data("heart", package = "kmed"))


# Kalp hastasÄ±, kalp hastasÄ± deÄŸil dÃ¶nÃ¼ÅŸÃ¼mÃ¼ yapÄ±lmÄ±ÅŸtÄ±r.
df %<>% mutate(class = ifelse(df$class == 0, 0,1)) 
df2 <- df

# gerekli dÃ¶nÃ¼ÅŸÃ¼mler:

head(df)
str(df)
getwd()
df$sex <- as.numeric(df$sex)
df$sex <- as.factor(df$sex)
df$fbs <- as.numeric(df$fbs)
df$fbs <- as.factor(df$fbs)
df$exang <- as.numeric(df$exang)
df$exang <- as.factor(df$exang)
df$ca <- as.factor(df$ca)
df$class <- as.factor(df$class)


# gerekli dÃ¶nÃ¼ÅŸÃ¼mler sonrasÄ± veri setindeki deÄŸiÅŸkenlerin tÃ¼rleri
str(df)


sum(is.na(df))
# veri setinde hiÃ§ eksik deÄŸer bulunmamakta.

#################################
### TanÄ±mlayÄ±cÄ± Ä°statistikler ###
#################################

summary(df)


# Veri setinde yer alan sayÄ±sal deÄŸerlerin tanÄ±mlayÄ±cÄ± istatistiklerini incelediÄŸimizde:

# Age deÄŸiÅŸkeninin ortalamasÄ±nÄ±n medyandan daha dÃ¼ÅŸÃ¼k olduÄŸu saptanmÄ±ÅŸtÄ±r. Bu da deÄŸiÅŸkenin sola Ã§arpÄ±k olduÄŸunu gÃ¶stermektedir. Birinci kartil ile minimum deÄŸer arasÄ±ndaki farka bakÄ±ldÄ±ÄŸÄ±nda uÃ§ deÄŸerlerin olabileceÄŸi dÃ¼ÅŸÃ¼nÃ¼lmÃ¼ÅŸtÃ¼r.
# Trestbps deÄŸiÅŸkeninin ortalamasÄ±nÄ±n medyanÄ±ndan az da olsa bÃ¼yÃ¼k olduÄŸu saptanmÄ±ÅŸtÄ±r. Bu da deÄŸiÅŸkenin saÄŸa Ã§arpÄ±k olduÄŸunu gÃ¶stermektedir. Kartiller ile min max deÄŸiÅŸkenleri incelendiÄŸinde ise aykÄ±rÄ± gÃ¶zlemlerin olabileceÄŸi dÃ¼ÅŸÃ¼nÃ¼lmÃ¼ÅŸtÃ¼r.
# chol deÄŸiÅŸkeninin ortalamasÄ±nÄ±n medyandan daha bÃ¼yÃ¼k olduÄŸu saptanmÄ±ÅŸtÄ±r. Bu da deÄŸiÅŸkenin saÄŸa Ã§arpÄ±k olduÄŸunu gÃ¶stermektedir. Kartiller ile min-max deÄŸerleri incelendiÄŸinde ise aykÄ±rÄ± gÃ¶zlemler olabileceÄŸi dÃ¼ÅŸÃ¼nÃ¼lmektedir.
# thalach deÄŸiÅŸkeninin medyanÄ±nÄ±n ortalamadan daha bÃ¼yÃ¼k olduÄŸu gÃ¶rÃ¼lmÃ¼ÅŸtÃ¼r. Bu da deÄŸiÅŸkenin sola Ã§arpÄ±k olduÄŸunu gÃ¶stermektedir. Kartiller ile min-max deÄŸerleri arasÄ±ndaki fark incelendiÄŸinde ise aykÄ±rÄ± gÃ¶zlem olabileceÄŸi dÃ¼ÅŸÃ¼nÃ¼lmektedir.
# AykÄ±rÄ± gÃ¶zlemler iÃ§in boxplot, daÄŸÄ±lÄ±mlar ile ilgili genel bir bilgi sahibi olabilmek iÃ§in ise histogram grafiklerine baÅŸvurulacaktÄ±r.


# Veri setinde yer alan kategorik deÄŸerlerin tanÄ±mlayÄ±cÄ± istatistikleri incelendiÄŸinde:

# Sex deÄŸiÅŸkeni incelendiÄŸinde ise veride yer alan gÃ¶zlemlerin bÃ¼yÃ¼k bir Ã§oÄŸunluÄŸunun erkek olduÄŸu saptanmÄ±ÅŸtÄ±r.
# cp deÄŸiÅŸkeni incelendiÄŸinde Ã§oÄŸunluÄŸun asymptomatic tÃ¼rÃ¼ndeki gÃ¶ÄŸÃ¼s aÄŸrÄ±sÄ± olduÄŸu saptanmÄ±ÅŸtÄ±r.
# fbs deÄŸiÅŸkeni incelendiÄŸinde gÃ¶zlemlerin bÃ¼yÃ¼k bir Ã§oÄŸunluÄŸunun 120mg/dl'den daha az kan ÅŸekeri olduÄŸu saptanmÄ±ÅŸtÄ±r.
# restecg deÄŸiÅŸkeni incelendiÄŸinde gÃ¶zlemlerin normal ile olasÄ± elektrokardiografik sonuÃ§larÄ± olduÄŸu, Ã§ok az kiÅŸinin anormal olduÄŸu saptanmÄ±ÅŸtÄ±r.
# exang deÄŸiÅŸkeni incelendiÄŸinde gÃ¶zlemlerin bÃ¼yÃ¼k bir Ã§oÄŸunluÄŸunda anjin gÃ¶rÃ¼lmediÄŸi saptanmÄ±ÅŸtÄ±r.
# slope deÄŸiÅŸkeni incelendiÄŸinde gÃ¶zlemlerin bÃ¼yÃ¼k bir Ã§oÄŸunluÄŸunda egzersiz ST segmentinin eÄŸiminin dÃ¼z olduÄŸu saptanmÄ±ÅŸtÄ±r.
# ca deÄŸiÅŸkeni incelendiÄŸinde gÃ¶zlemlerin bÃ¼yÃ¼k bir Ã§oÄŸunluÄŸunun 0 deÄŸerini aldÄ±ÄŸÄ± saptanmÄ±ÅŸtÄ±r.
# thal deÄŸiÅŸkeni incelendiÄŸinde gÃ¶zlemlerin bÃ¼yÃ¼k bir Ã§oÄŸunluÄŸunun normal ve reversable defect seviyelerini aldÄ±ÄŸÄ± gÃ¶zlemlenmiÅŸtir.
# baÄŸÄ±mlÄ± deÄŸiÅŸken class incelendiÄŸinde ise 160 kiÅŸinin kalp krizi geÃ§irmediÄŸi, 137 kiÅŸinin ise kalp krizi geÃ§irdiÄŸi saptanmÄ±ÅŸtÄ±r. 

### GÃ¶rsel Analizler ###

par(mfrow = c(1,5), bty = "n")

boxplot(df$age, col = "goldenrod1", main = "Age", border = "firebrick3")
boxplot(df$trestbps, col = "goldenrod1" ,main = "Trestbps", border = "firebrick3")
boxplot(df$chol, col = "goldenrod1", main = "Chol", border = "firebrick3")
boxplot(df$thalach, col = "goldenrod1", main = "Thalach", border = "firebrick3")
boxplot(df$oldpeak, col = "goldenrod1", main = "Oldpeak", border = "firebrick3")

# SayÄ±sal deÄŸiÅŸkenlerin kutu grafikleri incelendiÄŸinde:

# Age deÄŸiÅŸkeninde herhangi bir aykÄ±rÄ± gÃ¶zlem gÃ¶rÃ¼lmemektedir. Sola Ã§arpÄ±klÄ±k yine dikkat Ã§ekmektedir. Range'i ise oldukÃ§a yÃ¼ksek gÃ¶rÃ¼lmektedir.
# Trestbps deÄŸiÅŸkeni incelendiÄŸinde birÃ§ok aykÄ±rÄ± gÃ¶zlem olduÄŸu saptanmÄ±ÅŸtÄ±r. 
# Chol deÄŸiÅŸkeni incelendiÄŸinde 5 adet aykÄ±rÄ± gÃ¶zlem saptanmÄ±ÅŸtÄ±r.
# Thalach deÄŸiÅŸkeni incelendiÄŸinde 1 adet aykÄ±rÄ± gÃ¶zlem saptanmÄ±ÅŸtÄ±r.
# Oldpeak deÄŸiÅŸkeninde 4 adet aykÄ±rÄ± gÃ¶zlem dikkat Ã§ekmektedir. 


indexes = sapply(df, is.numeric)
indexes["class"] = TRUE
df[,indexes]%>%
  gather(-class, key = "var", value = "value") %>% 
  ggplot(aes(x = value, y = class, color = class)) +
  geom_boxplot() +
  facet_wrap(~ var, scales = "free")+
  theme(axis.text.x = element_text(angle = 30, hjust = 0.85),legend.position="none",
        panel.background = element_rect(fill = "white"))+
  theme(strip.background =element_rect(fill="goldenrod1"))+
  theme(strip.text = element_text(colour = "firebrick3"))

# SayÄ±sal deÄŸiÅŸkenlerin baÄŸÄ±mlÄ± deÄŸiÅŸkenin seviyelerine gÃ¶re kutu grafikleri incelendiÄŸinde:

# Kalp krizi geÃ§irmeyen gÃ¶zlemlerin daha geniÅŸ bir rangede olduÄŸu saptanmÄ±ÅŸtÄ±r.
# Kalp krizi geÃ§iren gÃ¶zlemlerin yaÅŸ ortalamasÄ±nÄ±n geÃ§irmeyenlere gÃ¶re daha fazla olduÄŸu saptanmÄ±ÅŸtÄ±r.
# Ä°lginÃ§ bir ÅŸekilde kolesterol bilgisi iÃ§eren deÄŸiÅŸken iÃ§in class deÄŸiÅŸkeninin seviyelerine gÃ¶re fark edilir bir deÄŸiÅŸim bulunmamaktadÄ±r.
# Maximum kolesterole sahip bireyin kalp krizi geÃ§irmemiÅŸ olmasÄ± da ilginÃ§ olarak ifade edilebilir. 
# Dinlenmeye gÃ¶re egzersizin neden olduÄŸu ST depresyonu bilgisini iÃ§eren oldpeak deÄŸiÅŸkeni incelendiÄŸinde kalp krizi geÃ§iren bireylerin daha yÃ¼ksek deÄŸerlerde olduÄŸu saptanmÄ±ÅŸtÄ±r.
# UlaÅŸÄ±lan maksimum kalp atÄ±ÅŸ hÄ±zÄ± bilgisini iÃ§eren thalach deÄŸiÅŸkeni incelendiÄŸinde kalp krizi geÃ§irmeyen bireylerin daha yÃ¼ksek kalp atÄ±ÅŸÄ± hÄ±zÄ±na ulaÅŸtÄ±ÄŸÄ± saptanmÄ±ÅŸtÄ±r. Kalp krizi geÃ§iren gÃ¶zlemlerin daha geniÅŸ bir aralÄ±kta olduÄŸu saptanÄ±rken daha dÃ¼ÅŸÃ¼k deÄŸerleri aldÄ±klarÄ± da saptanmÄ±ÅŸtÄ±r.
# Ä°stirahat halindeki kan basÄ±ncÄ± bilgisini iÃ§eren trestbps deÄŸiÅŸkeni incelendiÄŸinde ise kalp krizi geÃ§irenler ile geÃ§irmeyenlerin ortalamalarÄ± arasÄ±nda bir fark gÃ¶rÃ¼lmemektedir. Ancak kalp krizi geÃ§irenlerin biraz daha yÃ¼ksek deÄŸerler aldÄ±ÄŸÄ± sÃ¶ylenebilir. 

############################
### EÄŸitim - Test AyrÄ±mÄ± ###
############################

smp_size <- floor(0.70 * nrow(df)) 
set.seed(2021900444) 
train_ind <- sample(nrow(df), size = smp_size, replace = FALSE)
train <- df[train_ind, ]
test <- df[-train_ind, ]

##############################################################
### Denetimli Ä°statistiksel Ã–ÄŸrenme Modellerinin KurulmasÄ± ###
##############################################################

###########################
### SÄ±nÄ±flandÄ±rma AÄŸacÄ± ###
###########################


### Tree Paketi ile ###

treeclass <- tree(class~. , train )
summary(treeclass ) # error rate Ã¶nemli

# Toplam 18 terminal node ile aÄŸaÃ§ oluÅŸturulmuÅŸ.
# Residual mean deviance 0.4224 olarak dikkat Ã§ekiyor.
# Error rate 0.09 gibi dÃ¼ÅŸÃ¼k sayÄ±labilecek bir deÄŸer almÄ±ÅŸ.
dev.off()

plot(treeclass )
text(treeclass ,pretty =0)

# KÃ¶k dÃ¼ÄŸÃ¼m(root node) cp'nin 1,2 ve 3 olmasÄ± olarak saptanmÄ±ÅŸ.
# age'in 66.5'tan kÃ¼Ã§Ã¼k olmasÄ± son dÃ¼ÄŸÃ¼mlerden(terminal node) biri olarak saptanmÄ±ÅŸ. Ancak her iki node iÃ§in de deÄŸiÅŸiklik olmamasÄ± dikkat Ã§ekiyyor
# DiÄŸer terminal node'larda da benzer bir durum var. AÄŸacÄ±n budanmasÄ± gerektiÄŸi aÃ§Ä±kÃ§a belli oluyor.
# Age'in 55.5'tenn dÃ¼ÅŸÃ¼k olmasÄ± iÃ§ dÃ¼ÄŸÃ¼mlerden(internal node) biri olarak saptanmÄ±ÅŸ.
# Terminal nodelarÄ±n bÃ¼yÃ¼k bir Ã§oÄŸunluÄŸunda aynÄ± deÄŸerler dikkat Ã§ekiyor. Bu da prune etmenin gerekliliÄŸini vurguluyor.

set.seed(3)
cv.treeclass <- cv.tree(treeclass ,FUN=prune.misclass )
plot(cv.treeclass$size ,cv.treeclass$dev ,type="b")


# Her iki grafik de incelendiÄŸinde size'Ä±n 4 olduÄŸu noktada deviance'de belirgin azalmalar dikkat Ã§ekmekte. 
# Bununla birlikte 5,6,7,8 iÃ§in de aynÄ± durum geÃ§erli olmasÄ±na raÄŸmen, 10'da bir artÄ±ÅŸ gÃ¶zlemleniyor. 10'dan sonra dÃ¼ÅŸÃ¼ÅŸ tekrar gerÃ§ekleÅŸmiÅŸ.
# Bu sebeple hem ilk dÃ¼ÅŸÃ¼ÅŸ olan 4 hem de ikinci dÃ¼ÅŸÃ¼ÅŸ olan 12 deÄŸerleri iÃ§in iki adet budama yapÄ±lma kararÄ± alÄ±nmÄ±ÅŸtÄ±r.

### Budama 1

prune.treeclass1 <- prune.misclass (treeclass,best=4)
summary(prune.treeclass1)

# Budama sonrasÄ± yalnÄ±zca dÃ¶rt node ile model kurulurken residual mean deviance'Ä±nÄ±n 0.82 olarak saptanmÄ±ÅŸ. Bir artÄ±ÅŸ gÃ¶stermiÅŸ gÃ¶rÃ¼nÃ¼yor.
# Error rate ise 0.1594 deÄŸerini almÄ±ÅŸ. Misclassification error rate iÃ§in de bir artÄ±ÅŸ gÃ¶rÃ¼nÃ¼yor.

dev.off()
plot(prune.treeclass1 )
text(prune.treeclass1 ,pretty =0)

# YalnÄ±zca dÃ¶rt terminal node olduÄŸu iÃ§in Ã§ok verimli bir aÄŸaÃ§ olduÄŸu sÃ¶ylenemez. 
# AÄŸaÃ§ yalnÄ±zca cp, cave thalach deÄŸiÅŸkenleri kullanÄ±larak oluÅŸturulmuÅŸ.


### Budama 2

prune.treeclass2 <- prune.misclass (treeclass,best=12)
summary(prune.treeclass2)

# Budama sonrasÄ± 12 node ile model kurulurken residual mean deviance'Ä±nÄ±n 0.5 olarak saptanmÄ±ÅŸ. Ä°lk aÄŸaca gÃ¶re bir artÄ±ÅŸ gÃ¶stermiÅŸ gÃ¶rÃ¼nÃ¼yor. Ancak ilk budanmÄ±ÅŸ haline gÃ¶re daha dÃ¼ÅŸÃ¼k bir deÄŸer.
# Error rate ise 0.09179 deÄŸerini almÄ±ÅŸ. Misclassification error rate iÃ§in budanmamÄ±ÅŸ aÄŸaÃ§ ile aynÄ± deÄŸeri almÄ±ÅŸ.

dev.off()
plot(prune.treeclass2 )
text(prune.treeclass2 ,pretty =0)

# AÄŸaÃ§ cp, age, thalach, testbps, sex, ca, exang, chol ve restecg deÄŸiÅŸkenleri kullanÄ±larak oluÅŸturulmuÅŸ.
# Terminal nodelarda aynÄ± sÄ±nÄ±ftan ayrÄ±ÅŸmalar gÃ¶rÃ¼nmÃ¼yor. Bu sebeple budanmamÄ±ÅŸ aÄŸaca gÃ¶re daha doÄŸru bir ayrÄ±m olduÄŸu sÃ¶ylenebilir. 



### Budama Ã–ncesi Tahminler ###

### Train

classtree.pred <- predict(treeclass ,train ,type="class")

a<-caret::confusionMatrix(classtree.pred, train$class)  
a$overall[1]
# Accuracy Rate : 0.9082
# Sensitivity : 0.8981
# Specificity : 0.9192

ctpredictions <- data.frame()
ctpredictions[1,1] <- "Budama Ã–ncesi CT"
ctpredictions[1,2] <- "Train"
ctpredictions[1,3] <- a$overall[1]
ctpredictions[1,4] <- a$byClass[1]
ctpredictions[1,5] <- a$byClass[2]


### Test

classtree.predtest <- predict(treeclass, test, type = "class")

a <- caret::confusionMatrix(classtree.predtest, test$class)

# Accuracy Rate : 0.6889
# Sensitivity : 0.7308
# Specificity : 0.6316

ctpredictions[2,1] <- "Budama Ã–ncesi CT"
ctpredictions[2,2] <- "Test"
ctpredictions[2,3] <- a$overall[1]
ctpredictions[2,4] <- a$byClass[1]
ctpredictions[2,5] <- a$byClass[2]


### Ä°lk Budama SonrasÄ± Tahminler ###

### Train

prunedtree.pred1 <- predict(prune.treeclass1 ,train ,type="class")

a <- caret::confusionMatrix(prunedtree.pred1, train$class)

# Accuracy Rate : 0.8406
# Sensitivity : 0.9259
# Specificity : 0.7475

ctpredictions[3,1] <- "Ä°lk Budama CT"
ctpredictions[3,2] <- "Train"
ctpredictions[3,3] <- a$overall[1]
ctpredictions[3,4] <- a$byClass[1]
ctpredictions[3,5] <- a$byClass[2]


### Test

prunedtree.predtest1 <- predict(prune.treeclass1, test, type = "class")

a <- caret::confusionMatrix(prunedtree.predtest1, test$class)

# Accuracy Rate : 0.711
# Sensitivity : 0.8654
# Specificity : 0.5

ctpredictions[4,1] <- "Ä°lk Budama CT"
ctpredictions[4,2] <- "Test"
ctpredictions[4,3] <- a$overall[1]
ctpredictions[4,4] <- a$byClass[1]
ctpredictions[4,5] <- a$byClass[2]


### Ä°kinci Budama SonrasÄ± Tahminler ###

### Train

prunedtree.pred2 <- predict(prune.treeclass2 ,train ,type="class")

a <- caret::confusionMatrix(prunedtree.pred2, train$class)

# Accuracy Rate : 0.9082
# Sensitivity : 0.9074
# Specificity : 0.9091

ctpredictions[5,1] <- "Ä°kinci Budama CT"
ctpredictions[5,2] <- "Train"
ctpredictions[5,3] <- a$overall[1]
ctpredictions[5,4] <- a$byClass[1]
ctpredictions[5,5] <- a$byClass[2]

### Test

prunedtree.predtest2 <- predict(prune.treeclass2, test, type = "class")

a <- caret::confusionMatrix(prunedtree.predtest2, test$class)

# Accuracy Rate : 0.7111
# Sensitivity : 0.7692
# Specificity : 0.6316

ctpredictions[6,1] <- "Ä°kinci Budama CT"
ctpredictions[6,2] <- "Test"
ctpredictions[6,3] <- a$overall[1]
ctpredictions[6,4] <- a$byClass[1]
ctpredictions[6,5] <- a$byClass[2]

########################################################### rpart paketi ile ############################################################

# bu paketin Ã¶zelliÄŸi cross validationÄ±nÄ± da kendisinin yapÄ±p en az hatanÄ±n olduÄŸu budanmÄ±ÅŸ aÄŸacÄ± otomatik olarak oluÅŸturmasÄ±dÄ±r.

treeclass2 <- rpart(class~., data = train, method = 'class')

treeclass2$variable.importance

# deÄŸiÅŸkenlerin Ã¶nemi sÄ±ralandÄ±ÄŸÄ±nda en Ã¶nemli deÄŸiÅŸken cp olarak dikkat Ã§ekiyor.
# ArdÄ±ndan thalach, thal, exang deÄŸiÅŸkenleri geliyor.

treeclass2$numresp
# aÄŸaÃ§ dÃ¶rt baÄŸÄ±msÄ±z deÄŸiÅŸken kullanÄ±larak oluÅŸturulmuÅŸ.

rpart.plot(treeclass2)

# aÄŸacÄ± incelediÄŸimizde kÃ¶k dÃ¼ÄŸÃ¼m olarak yine cp'nin 1,2,3 olmasÄ± dikkat Ã§ekiyor.
# internal node'lar ca'nÄ±n sÄ±fÄ±ra eÅŸit olmasÄ±, thalach'Ä±n 146'dan bÃ¼yÃ¼keÅŸit olmasÄ± durumlarÄ± olarak dikkat Ã§ekiyor.
# toplam 7 adet terminal node bulunuyor. her bir atamayÄ± farklÄ± renklere ayÄ±rararak gÃ¶stermiÅŸ. renklerin tonu ise iÃ§erdiÄŸi gÃ¶zlem miktarÄ±nÄ± iÅŸaret ediyor.



### Tahminler ###

preds <- data.frame()

preds[1,1] <- "ClassTree"
preds[1,2] <- "Train"

### Train

prunedtree.pred3 <- predict(treeclass2 ,train ,type="class")

a <- caret::confusionMatrix(prunedtree.pred3, train$class)

# Accuracy Rate : 0.8744
# Sensitivity : 0.9444
# Specificity : 0.7980

preds[1,3] <- 0.8744 
preds[1,4] <- 0.9444
preds[1,5] <- 0.7980

ctpredictions[7,1] <- "rpart CT"
ctpredictions[7,2] <- "Train"
ctpredictions[7,3] <- a$overall[1]
ctpredictions[7,4] <- a$byClass[1]
ctpredictions[7,5] <- a$byClass[2]

### Test

prunedtree.predtest3 <- predict(treeclass2, test, type = "class")

a <- caret::confusionMatrix(prunedtree.predtest3, test$class)

# Accuracy Rate : .78
# Sensitivity : 0.9038
# Specificity : 0.6316



ctpredictions[8,1] <- "rpart CT"
ctpredictions[8,2] <- "Test"
ctpredictions[8,3] <- a$overall[1]
ctpredictions[8,4] <- a$byClass[1]
ctpredictions[8,5] <- a$byClass[2]

names(ctpredictions) <- c("Algoritma", "TT", "Accuracy_Rate", "Sensivity", "Specificity" )



ctpredictions %>% 
  ggplot(aes(x= Accuracy_Rate, y= reorder(Algoritma, -Accuracy_Rate))) +
  geom_line(stat="identity") +
  geom_point(aes(color=TT), size=3) +
  theme(legend.position="top") +
  theme(panel.background = element_rect(fill="white"))+
  xlab("Accuracy Rate") +
  ylab("Algoritma") 

ctpredictions %>% 
  ggplot(aes(x= Sensivity, y= reorder(Algoritma, -Sensivity))) +
  geom_line(stat="identity") +
  geom_point(aes(color=TT), size=3) +
  theme(legend.position="top") +
  theme(panel.background = element_rect(fill="white"))+
  xlab("Sensitivity") +
  ylab("Algoritma")


ctpredictions %>% 
  ggplot(aes(x= Specificity, y= reorder(Algoritma, -Specificity))) +
  geom_line(stat="identity") +
  geom_point(aes(color=TT), size=3) +
  theme(legend.position="top") +
  theme(panel.background = element_rect(fill="white"))+
  xlab("Specificity") +
  ylab("Algoritma")

# En nihayetinde bizim iÃ§in Ã¶nemli olan sensivity deÄŸerleri olacaktÄ±r. Ã‡Ã¼nkÃ¼ kalp krizi geÃ§irmiÅŸ birini kalp krizi geÃ§irmiÅŸ olarak tahmin etmesi hayati bir Ã¶nem taÅŸÄ±maktadÄ±r.
# Bu sebeple algoritmalar arasÄ±nda seÃ§im yaparken yÃ¼ksek sensitivity deÄŸeri vermiÅŸ olan rpart ile oluÅŸturulmuÅŸ aÄŸaÃ§ Ã¶ne Ã§Ä±kmaktadÄ±r.
# train ile test arasÄ±ndaki ayrÄ±mÄ±n da daha az olmasÄ± bu seÃ§ime sebep olmuÅŸtur.
# ROC curve Ã§izdirilirken rpart paketi ile oluÅŸturulan classification tree kullanÄ±lacaktÄ±r.

##############################################################################################################################################

###############
### Bagging ###
###############


### Random Forest Paketi ile ###
set.seed(125) 
bag <- randomForest(class~. , data=train, mtry=13,importance=TRUE)

bag

# Toplam 500 aÄŸaÃ§ kullanÄ±larak model kurulmuÅŸ.
# Her bir ayrÄ±mda 13 adet deÄŸiÅŸken kullanÄ±lmÄ±ÅŸ.
# OOB hata oranÄ± ise %18.84 olarak saptanmÄ±ÅŸtÄ±r.

bag$importance
varImpPlot(bag)

# deÄŸiÅŸkenlerin Ã¶nemlerini iÅŸaret eden grafik incelendiÄŸinde proline deÄŸiÅŸkeninin meandecreaseaccuracy deÄŸerine gÃ¶re Ã¶nemli deÄŸiÅŸkenler:
# ca, cp,, thalach, oldpeak, thal
# dÃ¼ÄŸÃ¼m saflÄ±ÄŸÄ±nÄ± iÅŸaret eden gini deÄŸerine gÃ¶re Ã¶nemli deÄŸiÅŸkenler:
# cp, thalach, ca, oldpeak, thal, age


### ipred paketi ile ###

# Modele kaÃ§ iterasyonun dahil edileceÄŸini kontrol etmek iÃ§in nbagg kullanÄ±lÄ±r 
# coob = TRUE OOB hata oranÄ±nÄ± kullanmayÄ± gÃ¶stermektedir. 
# tr control argÃ¼mani ile 10-fold cross validation fonksiyonun iÃ§inde uygulanÄ±r

bag2 <- bagging(
  formula = class ~ .,
  data = train,
  nbagg = 500,  
  coob = TRUE,
  method = "treebag",
  trControl = trainControl(method = "cv", number = 10))


bag2$err

# OOB Missclassification error rate 0.1642 olarak saptanmÄ±ÅŸtÄ±r.

VI <- data.frame(var=names(train[,-14]), imp=varImp(bag2))

VI_plot <- VI[order(VI$Overall, decreasing=F),]

barplot(VI_plot$Overall,
        names.arg=rownames(VI_plot),
        horiz=T,
        col="goldenrod1",
        xlab="Variable Importance",
        las = 2)
# DeÄŸiÅŸkenlerin Ã¶nemini ifade eden grafiÄŸi incelediÄŸimizde bir Ã¶nceki paketten daha farklÄ± bir grafikle karÅŸÄ±laÅŸÄ±lmÄ±ÅŸtÄ±r.
# ca ve cp deÄŸiÅŸkenleri diÄŸer pakette en Ã¶nemli deÄŸiÅŸkenler olarak gÃ¶rÃ¼nÃ¼rken bu sefer thalach deÄŸiÅŸkeninin daha Ã¶nemli olduÄŸu fark edilmiÅŸtir.
# thalach deÄŸiÅŸkenini ise cp, ca, thal, old peak, exang ve age deÄŸiÅŸkenlerinin takip ettiÄŸi sÃ¶ylenebilir.

bagpred <- data.frame()

### Tree Paketi ile OluÅŸturulan Modelin Tahminleri ###

### Train

baggintrain <- predict(bag ,train ,type="class")

a <- caret::confusionMatrix(baggintrain, train$class)

# Accuracy Rate : 1
# Sensitivity : 1
# Specificity : 1


### Test

baggintest <- predict(bag, test, type = "class")

b <- caret::confusionMatrix(baggintest, test$class)

# Accuracy Rate : 0.8
# Sensitivity : 0.9038
# Specificity : 0.6579

bagpred <- data.frame()
bagpred[1,1] <- "bagmodel1"
bagpred[1,2] <- "train"
bagpred[2,2] <- "test"
bagpred[2,1] <- "bagmodel1"
bagpred[1,3] <- a$overall[1]
bagpred[2,3] <- b$overall[1]
bagpred[1,4] <- a$byClass[1]
bagpred[2,4] <- b$byClass[1]
bagpred[1,5] <- a$byClass[2]
bagpred[2,5] <- b$byClass[2]


### ipred ile oluÅŸturulan Modelin Tahminleri

### Train

baggintrain1 <- predict(bag2 ,train ,type="class")

a <- caret::confusionMatrix(baggintrain1, train$class)

# Accuracy Rate : 1
# Sensitivity : 1
# Specificity : 1

### Test

baggintest1 <- predict(bag2, test, type = "class")

b <- caret::confusionMatrix(baggintest1, test$class)

# Accuracy Rate : 0.81
# Sensitivity : 0.9231
# Specificity : 0.6579


bagpred[3,1] <- "ipredmodel"
bagpred[3,2] <- "train"
bagpred[4,2] <- "test"
bagpred[4,1] <- "ipredmodel"
bagpred[3,3] <- a$overall[1]
bagpred[4,3] <- b$overall[1]
bagpred[3,4] <- a$byClass[1]
bagpred[4,4] <- b$byClass[1]
bagpred[3,5] <- a$byClass[2]
bagpred[4,5] <- b$byClass[2]



names(bagpred) <- c("Algoritma", "TT", "Accuracy_Rate", "Sensivity", "Specificity" )

bagpred %>% 
  ggplot(aes(x= Accuracy_Rate, y= reorder(Algoritma, -Accuracy_Rate))) +
  geom_line(stat="identity") +
  geom_point(aes(color=TT), size=3) +
  theme(legend.position="top") +
  theme(panel.background = element_rect(fill="white"))+
  xlab("Accuracy Rate") +
  ylab("Algoritma") 

bagpred %>% 
  ggplot(aes(x= Sensivity, y= reorder(Algoritma, -Sensivity))) +
  geom_line(stat="identity") +
  geom_point(aes(color=TT), size=3) +
  theme(legend.position="top") +
  theme(panel.background = element_rect(fill="white"))+
  xlab("Sensitivity") +
  ylab("Algoritma")

bagpred %>% 
  ggplot(aes(x= Specificity, y= reorder(Algoritma, -Specificity))) +
  geom_line(stat="identity") +
  geom_point(aes(color=TT), size=3) +
  theme(legend.position="top") +
  theme(panel.background = element_rect(fill="white"))+
  xlab("Specificity") +
  ylab("Algoritma")


# Classification Tree seÃ§iminde uygulanan aynÄ± mantÄ±k ile ROC curve iÃ§in seÃ§ilen model ipred paketi ile oluÅŸturulan paket olmuÅŸtur.

##############################################################################################################################################

#####################
### Random Forest ###
#####################

sqrt(13)

rf <- randomForest(class~. ,data=train, mtry=4,importance=TRUE)

rf$confusion

# SÄ±fÄ±rÄ±ncÄ± sÄ±nÄ±f iÃ§in hata 0.14, birinci sÄ±nÄ±f iÃ§in ise 0.18 olarak saptanmÄ±ÅŸ. Toplam 34 gÃ¶zlem yanlÄ±ÅŸ olarak sÄ±nÄ±flandÄ±rÄ±lmÄ±ÅŸ.


rf 
# OOB estimate error rate ise %16.43 Ã§Ä±kmÄ±ÅŸ. Bunun Ã§ok olduÄŸu sÃ¶ylenilebilir.
# her bir ayrÄ±ÅŸmada 4 deÄŸiÅŸken denenmiÅŸ.
# Toplam 500 aÄŸaÃ§ kurulmuÅŸ.

which.max(rf$err.rate[,1]) 


rf$importance
varImpPlot(rf)

# DeÄŸiÅŸkenlerin Ã¶nemlerine bakÄ±ldÄ±ÄŸÄ±nda;

# Mean decrease accuracy incelendiÄŸinde en Ã§ok ca, ardÄ±ndan cp,thalach,thal, oldpeak sÄ±ralamasÄ± dikkat Ã§ekmekte.
# Gini deÄŸerlerine bakÄ±ldÄ±ÄŸÄ±nda ise thalach, cp, ca sÄ±ralamasÄ± dikkat Ã§ekiyor
# Baggin ile oldukÃ§a paralel olduÄŸunu sÃ¶ylemek yanlÄ±ÅŸ olmaz.

###################
### Grid Search ###
###################

# Grid search'te aÄŸaÃ§ sayÄ±sÄ± aralÄ±ÄŸÄ±na karar vermek iÃ§in grafik Ã§izdirilmiÅŸtir.
plot(rf)

hyper_grid <- expand.grid(
  mtry = c(3, 4, 5, 6),
  nodesize = c(1, 3, 5, 10), 
  numtrees = c(200, 220,300,330),
  rmse = NA                                               
)


for (i in 1:nrow(hyper_grid)) {
  fit <- randomForest(class~. ,
                      data=train, 
                      mtry=hyper_grid$mtry[i],
                      nodesize = hyper_grid$nodesize[i],
                      ntree = hyper_grid$numtrees[i],
                      importance=TRUE)
  hyper_grid$rmse[i] <- mean(fit$confusion[,3])
}

# assess top 10 models
hyper_grid %>%
  arrange(rmse) %>%
  head(10)

# BÃ¶ylelikle en iyi parametrelerle kurulacak model aÅŸaÄŸÄ±daki gibi olmalÄ±dÄ±r.

rf2 <- randomForest(class~. ,data=train, mtry=4,importance=TRUE, nodesize = 10, ntree= 220)

rf2$confusion

# SÄ±fÄ±rÄ±ncÄ± sÄ±nÄ±f iÃ§in hata 0.10, birinci sÄ±nÄ±f iÃ§in ise 0.18 olarak saptanmÄ±ÅŸ. Toplam 29 gÃ¶zlem yanlÄ±ÅŸ olarak sÄ±nÄ±flandÄ±rÄ±lmÄ±ÅŸ.

rf2

# OOB estimate error rate ise %14.01 Ã§Ä±kmÄ±ÅŸ. Bunun Ã§ok olduÄŸu sÃ¶ylenilebilir.
# her bir ayrÄ±ÅŸmada 4 deÄŸiÅŸken denenmiÅŸ.
# Toplam 220 aÄŸaÃ§ kurulmuÅŸ.


rf2$importance
varImpPlot(rf2)

# DeÄŸiÅŸkenlerin Ã¶nemlerine bakÄ±ldÄ±ÄŸÄ±nda;

# Mean decrease accuracy incelendiÄŸinde en Ã§ok ca, ardÄ±ndan cp, thalach, thal, oldpeak sÄ±ralamasÄ± dikkat Ã§ekmekte.
# Gini deÄŸerlerine bakÄ±ldÄ±ÄŸÄ±nda ise cp, ca, thalach, sÄ±ralamasÄ± dikkat Ã§ekiyor
# Ä°lk random forest modeline kÄ±yasla gini Ã¶nem sÄ±rasÄ±nda deÄŸiÅŸiklik gÃ¶zlemlenmiÅŸ.


### Ä°lk Model iÃ§in Tahminler ###

rfpred <- data.frame()
### Train

ranfortrain <- predict(rf ,train ,type="class")

a <- caret::confusionMatrix(ranfortrain, train$class)

# Accuracy Rate : 1
# Sensitivity : 1
# Specificity : 1

### Test

ranfortest <- predict(rf, test, type = "class")

b <- caret::confusionMatrix(ranfortest, test$class)

# Accuracy Rate : 0.8
# Sensitivity : 0.9038
# Specificity : 0.6579

rfpred <- data.frame()
rfpred[1,1] <- "rfmodel1"
rfpred[2,1] <- "rfmodel1"
rfpred[1,2] <- "train"
rfpred[2,2] <- "test"
rfpred[1,3] <- a$overall[1]
rfpred[2,3] <- b$overall[1]
rfpred[1,4] <- a$byClass[1]
rfpred[2,4] <- b$byClass[1]
rfpred[1,5] <- a$byClass[2]
rfpred[2,5] <- b$byClass[2]

### Grid Search SonrasÄ± Tahminler ###

### Train

ranfortrain1 <- predict(rf2 ,train ,type="class")

a <- caret::confusionMatrix(ranfortrain1, train$class)

# Accuracy Rate : 0.94
# Sensitivity : 0.9722
# Specificity : 0.9091



### Test

ranfortest1 <- predict(rf2, test, type = "class")

b <- caret::confusionMatrix(ranfortest1, test$class)

# Accuracy Rate : 0.7778
# Sensitivity : 0.9038
# Specificity : 0.6053

rfpred[3,1] <- "rfmodel2"
rfpred[4,1] <- "rfmodel2"
rfpred[3,2] <- "train"
rfpred[4,2] <- "test"
rfpred[3,3] <- a$overall[1]
rfpred[4,3] <- b$overall[1]
rfpred[3,4] <- a$byClass[1]
rfpred[4,4] <- b$byClass[1]
rfpred[3,5] <- a$byClass[2]
rfpred[4,5] <- b$byClass[2]



names(rfpred) <- c("Algoritma", "TT", "Accuracy_Rate", "Sensivity", "Specificity" )


rfpred %>% 
  ggplot(aes(x= Accuracy_Rate, y= reorder(Algoritma, -Accuracy_Rate))) +
  geom_line(stat="identity") +
  geom_point(aes(color=TT), size=3) +
  theme(legend.position="top") +
  theme(panel.background = element_rect(fill="white"))+
  xlab("Accuracy Rate") +
  ylab("Algoritma")


rfpred %>% 
  ggplot(aes(x= Sensivity, y= reorder(Algoritma, -Sensivity))) +
  geom_line(stat="identity") +
  geom_point(aes(color=TT), size=3) +
  theme(legend.position="top") +
  theme(panel.background = element_rect(fill="white"))+
  xlab("Sensitivity") +
  ylab("Algoritma")

rfpred %>% 
  ggplot(aes(x= Specificity, y= reorder(Algoritma, -Specificity))) +
  geom_line(stat="identity") +
  geom_point(aes(color=TT), size=3) +
  theme(legend.position="top") +
  theme(panel.background = element_rect(fill="white"))+
  xlab("Specificity") +
  ylab("Algoritma")

# Random forest modelleri arasÄ±nda grid search sonrasÄ± seÃ§ilen model en iyi model olarak seÃ§ilmiÅŸtir.



##############################################################################################################################################

###########################
### Logistic Regression ###
###########################

logmodel1 <- glm(class ~ age + sex + cp + trestbps + chol +
                   fbs + restecg + thalach + exang + oldpeak + slope + ca + thal, data = train, family = binomial)

summary(logmodel1)

# modelin anlamlÄ±lÄ±ÄŸÄ±

#ğ» 0 : ğ›½ 1 = ğ›½ 2 = â‹¯ = ğ›½ ğ‘˜ = 0
# ğ» 1 : En azÄ±ndan bir ğ›½ ğ‘— â‰  0

# G= Null deviance-Residual Deviance
286.57 - 118.63

1-pchisq(286.57 - 118.63,206-186) 

# Bu p-deÄŸeri .05'ten kÃ¼Ã§Ã¼k olduÄŸu iÃ§in sÄ±fÄ±r hipotezini reddebiliriz. 
# BaÅŸka bir deyiÅŸle, baÄŸÄ±msÄ±z deÄŸiÅŸkenlerin baÄŸÄ±mlÄ± deÄŸiÅŸkeni aÃ§Ä±klamada etkili olduÄŸunu sÃ¶yleyebilecek yeterli istatistiksel kanÄ±tÄ±mÄ±z bulunmaktadÄ±r. 


###
# katsayÄ± yorumu

# BaÄŸÄ±msÄ±z deÄŸiÅŸkenin deÄŸerini bir birim arttÄ±rdÄ±ÄŸÄ±mÄ±zda tahmin deÄŸerindeki deÄŸiÅŸikliÄŸi
# belirlemek iÃ§in Ã¶nce log(odds) formulÃ¼nde her iki tarafa exp fonksiyonu uygulanÄ±r.

# AnlamlÄ± deÄŸiÅŸkenlerin katsayÄ± yorumu:


exp(-3.279e-02) # age deÄŸiÅŸkendeki 1 birimlik artÄ±ÅŸ odds oranÄ±nÄ± 0.9677418 kat deÄŸiÅŸtirir.
exp(1.497e+00)  # sex1 deÄŸiÅŸkenindeki bir birimlik artÄ±ÅŸ odds oranÄ±nÄ± 4.468264 kat deÄŸiÅŸtirir
exp(2.293e+00)  # cp2 deÄŸiÅŸkenindeki bir birimlik artÄ±ÅŸ odds oranÄ±nÄ± 9.904607 kat deÄŸiÅŸtirir
exp(1.143e+00) # cp3 deÄŸiÅŸkenindeki 1 birimlik artÄ±ÅŸ odds oranÄ±nÄ± 3.136163 kat deÄŸiÅŸtirir.
exp(3.281e+00) # cp4 deÄŸiÅŸkenindeki 1 birimlik artÄ±ÅŸ odds oranÄ±nÄ± 26.60236 kat deÄŸiÅŸtirir.
exp(2.477e-02)  # trestbps deÄŸiÅŸkenindeki bir birimlik artÄ±ÅŸ odds oranÄ±nÄ± 1.025079 kat deÄŸiÅŸtirir
exp(5.457e-03)  # chol deÄŸiÅŸkenindeki bir birimlik artÄ±ÅŸ odds oranÄ±nÄ± 1.005472 kat deÄŸiÅŸtirir
exp(-4.559e-02) # fbs1 deÄŸiÅŸkenindeki 1 birimlik artÄ±ÅŸ odds oranÄ±nÄ± 0.9554336 kat deÄŸiÅŸtirir.
exp(1.306e+01) # restecg1 deÄŸiÅŸkenindeki 1 birimlik artÄ±ÅŸ odds oranÄ±nÄ± 469770.7 kat deÄŸiÅŸtirir.
exp(5.368e-02) # restecg2 deÄŸiÅŸkenindeki 1 birimlik artÄ±ÅŸ odds oranÄ±nÄ± 1.055147 kat deÄŸiÅŸtirir.
exp(-3.774e-02)  # thalach deÄŸiÅŸkenindeki bir birimlik artÄ±ÅŸ odds oranÄ±nÄ± 0.9629633 kat deÄŸiÅŸtirir
exp(6.948e-0) # exang1 deÄŸiÅŸkenindeki 1 birimlik artÄ±ÅŸ odds oranÄ±nÄ± 1041.066 kat deÄŸiÅŸtirir.
exp(3.551e-01) # oldpeak deÄŸiÅŸkenindeki 1 birimlik artÄ±ÅŸ odds oranÄ±nÄ± 1.426323 kat deÄŸiÅŸtirir.
exp(1.350e+00) # slope2 deÄŸiÅŸkenindeki 1 birimlik artÄ±ÅŸ odds oranÄ±nÄ± 3.857426 kat deÄŸiÅŸtirir.
exp(1.005e+00) # slope3 deÄŸiÅŸkenindeki 1 birimlik artÄ±ÅŸ odds oranÄ±nÄ± 2.731907 kat deÄŸiÅŸtirir.
exp(2.688e+00) # ca1 deÄŸiÅŸkenindeki 1 birimlik artÄ±ÅŸ odds oranÄ±nÄ± 14.70224 kat deÄŸiÅŸtirir.
exp(4.369e+00)  # ca2 deÄŸiÅŸkenindeki bir birimlik artÄ±ÅŸ odds oranÄ±nÄ± 78.96463 kat deÄŸiÅŸtirir
exp(2.833e+00) # ca3 deÄŸiÅŸkenindeki 1 birimlik artÄ±ÅŸ odds oranÄ±nÄ± 16.99637 kat deÄŸiÅŸtirir.
exp(-8.000e-01) # thal6 deÄŸiÅŸkenindeki 1 birimlik artÄ±ÅŸ odds oranÄ±nÄ± 0.449329 kat deÄŸiÅŸtirir.
exp(9.626e-01) # thal7 deÄŸiÅŸkenindeki 1 birimlik artÄ±ÅŸ odds oranÄ±nÄ± 2.618496 kat deÄŸiÅŸtirir.

###  KatsayÄ±lar iÃ§in gÃ¼ven aralÄ±ÄŸÄ± tahmini

### HÄ°POTEZ TESTÄ° EKLENECEK #####

confint.default(logmodel1)

# Î² katsayÄ±sÄ±na ait gÃ¼ven aralÄ±ÄŸÄ± sfÄ±r deÄŸerini iÃ§ermediÄŸi iÃ§in Ho hipotezi red edilerek aÅŸaÄŸÄ±daki katsayÄ±larÄ±n istatistiksel olarak anlamlÄ± olduÄŸuna karar verilmiÅŸtir.

# sex1, cp4, thalach, slope2, ca

# Î² katsayÄ±sÄ±na ait gÃ¼ven aralÄ±ÄŸÄ± sfÄ±r deÄŸerini iÃ§erdiÄŸi iÃ§in Ho hipotezi red edilemeyerek aÅŸaÄŸÄ±daki katsayÄ±larÄ±n istatistiksel olarak anlamlÄ± olmadÄ±ÄŸÄ±na karar verilmiÅŸtir.

# age, cp2, cp3, trestbps, chol, fbs1, restecg, exang, oldpeak, slope3, thal


# odds deÄŸeri  iÃ§in gÃ¼ven aralÄ±ÄŸÄ±

odds.confint <- exp(confint.default(logmodel1))
odds.confint

# Odds oran deÄŸerine ait gÃ¼ven aralÄ±ÄŸÄ± 1 deÄŸerini iÃ§ermediÄŸi iÃ§in Ho hipotezi red edilerek aÅŸaÄŸÄ±daki katsayÄ±larÄ±n istatistiksel olarak anlamlÄ± olduÄŸuna karar verilmiÅŸtir:

# age, sex, cp, trestbps, chol, fbs, restecg2, exang, oldpeak, slope, ca, thal

# AnlamlÄ± deÄŸiÅŸkenlerin odds deÄŸeri iÃ§in gÃ¼ven aralÄ±ÄŸÄ±na gÃ¶re yorumlanmasÄ± aÅŸaÄŸÄ±daki gibi olacaktÄ±r: 

cat("Age deÄŸiÅŸkeninin bir birim arttÄ±rÄ±lmasÄ± kalp krizi geÃ§irme oddsunu %95 gÃ¼venle Age deÄŸiÅŸkeninin bir birim dÃ¼ÅŸÃ¼k olana gÃ¶re ", odds.confint[2,1], " ile ", odds.confint[2,2], "katÄ± arasÄ±nda deÄŸer alÄ±r"  )
cat("Sex deÄŸiÅŸkeninin bir birim arttÄ±rÄ±lmasÄ± kalp krizi geÃ§irme oddsunu %95 gÃ¼venle Sex deÄŸiÅŸkeninin bir birim dÃ¼ÅŸÃ¼k olana gÃ¶re ", odds.confint[3,1], " ile ", odds.confint[3,2], "katÄ± arasÄ±nda deÄŸer alÄ±r"  )
cat("Cp2 deÄŸiÅŸkeninin bir birim arttÄ±rÄ±lmasÄ± kalp krizi geÃ§irme oddsunu %95 gÃ¼venle cp2 deÄŸiÅŸkeninin bir birim dÃ¼ÅŸÃ¼k olana gÃ¶re ", odds.confint[4,1], " ile ", odds.confint[4,2], "katÄ± arasÄ±nda deÄŸer alÄ±r"  )
cat("Cp3 deÄŸiÅŸkeninin bir birim arttÄ±rÄ±lmasÄ± kalp krizi geÃ§irme oddsunu %95 gÃ¼venle cp3 deÄŸiÅŸkeninin bir birim dÃ¼ÅŸÃ¼k olana gÃ¶re ", odds.confint[5,1], " ile ", odds.confint[5,2], "katÄ± arasÄ±nda deÄŸer alÄ±r"  )
cat("Cp4 deÄŸiÅŸkeninin bir birim arttÄ±rÄ±lmasÄ± kalp krizi geÃ§irme oddsunu %95 gÃ¼venle cp4 deÄŸiÅŸkeninin bir birim dÃ¼ÅŸÃ¼k olana gÃ¶re ", odds.confint[6,1], " ile ", odds.confint[6,2], "katÄ± arasÄ±nda deÄŸer alÄ±r"  )
cat("Trestbps deÄŸiÅŸkeninin bir birim arttÄ±rÄ±lmasÄ± kalp krizi geÃ§irme oddsunu %95 gÃ¼venle trestbps deÄŸiÅŸkeninin bir birim dÃ¼ÅŸÃ¼k olana gÃ¶re ", odds.confint[7,1], " ile ", odds.confint[7,2], "katÄ± arasÄ±nda deÄŸer alÄ±r"  )
cat("Chol deÄŸiÅŸkeninin bir birim arttÄ±rÄ±lmasÄ± kalp krizi geÃ§irme oddsunu %95 gÃ¼venle choldeÄŸiÅŸkeninin bir birim dÃ¼ÅŸÃ¼k olana gÃ¶re ", odds.confint[8,1], " ile ", odds.confint[8,2], "katÄ± arasÄ±nda deÄŸer alÄ±r"  )
cat("fbs1 deÄŸiÅŸkeninin bir birim arttÄ±rÄ±lmasÄ± kalp krizi geÃ§irme oddsunu %95 gÃ¼venle fbs1 deÄŸiÅŸkeninin bir birim dÃ¼ÅŸÃ¼k olana gÃ¶re ", odds.confint[9,1], " ile ", odds.confint[9,2], "katÄ± arasÄ±nda deÄŸer alÄ±r"  )
cat("restecg2 deÄŸiÅŸkeninin bir birim arttÄ±rÄ±lmasÄ± kalp krizi geÃ§irme oddsunu %95 gÃ¼venle restecg2 deÄŸiÅŸkeninin bir birim dÃ¼ÅŸÃ¼k olana gÃ¶re ", odds.confint[11,1], " ile ", odds.confint[11,2], "katÄ± arasÄ±nda deÄŸer alÄ±r"  )
cat("exang1 deÄŸiÅŸkeninin bir birim arttÄ±rÄ±lmasÄ± kalp krizi geÃ§irme oddsunu %95 gÃ¼venle exang1 deÄŸiÅŸkeninin bir birim dÃ¼ÅŸÃ¼k olana gÃ¶re ", odds.confint[12,1], " ile ", odds.confint[12,2], "katÄ± arasÄ±nda deÄŸer alÄ±r"  )
cat("oldpeak deÄŸiÅŸkeninin bir birim arttÄ±rÄ±lmasÄ± kalp krizi geÃ§irme oddsunu %95 gÃ¼venle oldpeak deÄŸiÅŸkeninin bir birim dÃ¼ÅŸÃ¼k olana gÃ¶re ", odds.confint[13,1], " ile ", odds.confint[13,2], "katÄ± arasÄ±nda deÄŸer alÄ±r"  )
cat("slope2 deÄŸiÅŸkeninin bir birim arttÄ±rÄ±lmasÄ± kalp krizi geÃ§irme oddsunu %95 gÃ¼venle slope2 deÄŸiÅŸkeninin bir birim dÃ¼ÅŸÃ¼k olana gÃ¶re ", odds.confint[14,1], " ile ", odds.confint[14,2], "katÄ± arasÄ±nda deÄŸer alÄ±r"  )
cat("slope3 deÄŸiÅŸkeninin bir birim arttÄ±rÄ±lmasÄ± kalp krizi geÃ§irme oddsunu %95 gÃ¼venle slope3 deÄŸiÅŸkeninin bir birim dÃ¼ÅŸÃ¼k olana gÃ¶re ", odds.confint[15,1], " ile ", odds.confint[15,2], "katÄ± arasÄ±nda deÄŸer alÄ±r"  )
cat("Ca1 deÄŸiÅŸkeninin bir birim arttÄ±rÄ±lmasÄ± kalp krizi geÃ§irme oddsunu %95 gÃ¼venle ca1 deÄŸiÅŸkeninin bir birim dÃ¼ÅŸÃ¼k olana gÃ¶re ", odds.confint[16,1], " ile ", odds.confint[16,2], "katÄ± arasÄ±nda deÄŸer alÄ±r"  )
cat("Ca2 deÄŸiÅŸkeninin bir birim arttÄ±rÄ±lmasÄ± kalp krizi geÃ§irme oddsunu %95 gÃ¼venle ca2 deÄŸiÅŸkeninin bir birim dÃ¼ÅŸÃ¼k olana gÃ¶re ", odds.confint[17,1], " ile ", odds.confint[17,2], "katÄ± arasÄ±nda deÄŸer alÄ±r"  )
cat("Ca3 deÄŸiÅŸkeninin bir birim arttÄ±rÄ±lmasÄ± kalp krizi geÃ§irme oddsunu %95 gÃ¼venle ca3 deÄŸiÅŸkeninin bir birim dÃ¼ÅŸÃ¼k olana gÃ¶re ", odds.confint[18,1], " ile ", odds.confint[18,2], "katÄ± arasÄ±nda deÄŸer alÄ±r"  )
cat("thal6 deÄŸiÅŸkeninin bir birim arttÄ±rÄ±lmasÄ± kalp krizi geÃ§irme oddsunu %95 gÃ¼venle thal6 deÄŸiÅŸkeninin bir birim dÃ¼ÅŸÃ¼k olana gÃ¶re ", odds.confint[19,1], " ile ", odds.confint[19,2], "katÄ± arasÄ±nda deÄŸer alÄ±r"  )
cat("thal7 deÄŸiÅŸkeninin bir birim arttÄ±rÄ±lmasÄ± kalp krizi geÃ§irme oddsunu %95 gÃ¼venle thal7 deÄŸiÅŸkeninin bir birim dÃ¼ÅŸÃ¼k olana gÃ¶re ", odds.confint[20,1], " ile ", odds.confint[20,2], "katÄ± arasÄ±nda deÄŸer alÄ±r"  )


### ArtÄ±klar

outlierTest(logmodel1)
# Bonferroni'ye gÃ¶re herhangi bir uÃ§ deÄŸer bulunmamakta.

### KaldÄ±raÃ§lar 
hvalues <- influence(logmodel1)$hat
r_si <- pearson.res.chd/(sqrt(1-hvalues))

# AÅŸaÄŸÄ±daki gÃ¶zlemler kaldÄ±raÃ§ noktasÄ± olarak saptanmÄ±ÅŸtÄ±r:
which(abs(r_si) > 2)


# Etkin GÃ¶zlem GrafiÄŸi
influencePlot(logmodel1)
# BÃ¼yÃ¼k mavi daire iÃ§erisinde alÄ±nan gÃ¶zlemler etkin gÃ¶zlem olarak dikkat Ã§ekmekte. 
# OldukÃ§a fazla olduÄŸu gÃ¶rÃ¼nÃ¼yor.


# SPECIFICITY AND SENSITIVITY

#DuyarlÄ±lÄ±k:
# GerÃ§ekte pozitif olanlarÄ±n iÃ§inde pozitif olarak teÅŸhis(tahmin) edilenlerin oranÄ± (TP/(TP+FN))

# Ã–zgÃ¼llÃ¼k:
# GerÃ§ekte negatif olanlarÄ±n iÃ§inde negatif olarak tahmin edilenlerin oranÄ± (TN/(TN+FN))

lgpred <- data.frame()

### Medyana gÃ¶re tahminler

ppred <- fitted(logmodel1)
summary(ppred)
# Ä°lk olarak threshold deÄŸeri medyan deÄŸeri kabul edilecek tahminlerde bulunulacaktÄ±r.
threshold <- 0.356234
ppred[ppred > threshold] <- 1
ppred[ppred < threshold] <- 0
ppred <- as.factor(ppred)
a <- caret::confusionMatrix(ppred, train$class)
# Accuracy Rate : 0.87
# Sensitivity : 0.86
# Specificity : 0.8889
### Test
testpred <- predict(logmodel1, newdata = test)
testpred[testpred > threshold] <- 1
testpred[testpred < threshold] <- 0
testpred <- as.factor(testpred)
b <- caret::confusionMatrix(testpred, test$class)
# Accuracy Rate : 0.83
# Sensitivity : 0.9038
# Specificity : 0.7368

lgpred <- data.frame()
lgpred[1,1] <- "lrmedyan"
lgpred[2,1] <- "lrmedyan"
lgpred[1,2] <- "train"
lgpred[2,2] <- "test"
lgpred[1,3] <- a$overall[1]
lgpred[2,3] <- b$overall[1]
lgpred[1,4] <- a$byClass[1]
lgpred[2,4] <- b$byClass[1]
lgpred[1,5] <- a$byClass[2]
lgpred[2,5] <- b$byClass[2]


### Meane gÃ¶re tahminler

ppred <- fitted(logmodel1)

threshold <- 0.4688995
ppred[ppred > threshold] <- 1
ppred[ppred < threshold] <- 0
ppred <- as.factor(ppred)
a <- caret::confusionMatrix(ppred, train$class)


# Accuracy Rate : 0.88
# Sensitivity : 0.91
# Specificity : 0.8586


### Test
testpred1 <- predict(logmodel1, newdata = test)
testpred1[testpred1 > threshold] <- 1
testpred1[testpred1 < threshold] <- 0
testpred1 <- as.factor(testpred1)
b <- caret::confusionMatrix(testpred1, test$class)


# Accuracy Rate : 0.83
# Sensitivity : 0.9038
# Specificity : 0.7368


lgpred[3,1] <- "lrmean"
lgpred[4,1] <- "lrmean"
lgpred[3,2] <- "train"
lgpred[4,2] <- "test"
lgpred[3,3] <- a$overall[1]
lgpred[4,3] <- b$overall[1]
lgpred[3,4] <- a$byClass[1]
lgpred[4,4] <- b$byClass[1]
lgpred[3,5] <- a$byClass[2]
lgpred[4,5] <- b$byClass[2]


### Threshold = 0.6'a gÃ¶re tahminler

ppred <- fitted(logmodel1)
summary(ppred)
threshold <- 0.6
ppred[ppred > threshold] <- 1
ppred[ppred < threshold] <- 0
ppred <- as.factor(ppred)
a <- caret::confusionMatrix(ppred, train$class)
# Accuracy Rate : 0.88
# Sensitivity : 0.9537
# Specificity : 0.8182



### Test
testpred1 <- predict(logmodel1, newdata = test)
testpred1[testpred1 > threshold] <- 1
testpred1[testpred1 < threshold] <- 0
testpred1 <- as.factor(testpred1)
b <- caret::confusionMatrix(testpred1, test$class)
# Accuracy Rate : 0.83
# Sensitivity : 0.9231
# Specificity : 0.7105

lgpred[5,1] <- "lr0.6"
lgpred[6,1] <- "lr0.6"
lgpred[5,2] <- "train"
lgpred[6,2] <- "test"
lgpred[5,3] <- a$overall[1]
lgpred[6,3] <- b$overall[1]
lgpred[5,4] <- a$byClass[1]
lgpred[6,4] <- b$byClass[1]
lgpred[5,5] <- a$byClass[2]
lgpred[6,5] <- b$byClass[2]


names(lgpred) <- c("Algoritma", "TT", "Accuracy_Rate", "Sensivity", "Specificity" )
# Accuracy Rate, sensitivity ve specificity deÄŸerlerinin mÃ¼mkÃ¼n olduÄŸunca yÃ¼ksek olmasÄ± gerekiyor.
# Ancak Ã§eÅŸitli denemeler sonucunda sensitivity deÄŸeri arttÄ±kÃ§a specificity deÄŸerinin azaldÄ±ÄŸÄ± gÃ¶zlemlenmiÅŸtir.
# Bu veri seti ve sÄ±nÄ±flandÄ±rmak istediÄŸimiz seviyeler dÃ¼ÅŸÃ¼nÃ¼ldÃ¼ÄŸÃ¼nde kalp krizi geÃ§irenlerin kalp krizi geÃ§irme tahmininin doÄŸruluÄŸunun daha Ã¶nemli olduÄŸu kanÄ±sÄ±na varÄ±lmÄ±ÅŸtÄ±r.
# Bu sebeple sensitivity sonucuna gÃ¶re en yÃ¼ksek olan son threshold, seÃ§im yapacaÄŸÄ±mÄ±z threshold olarak tercih edilmiÅŸtir.

lgpred %>% 
  ggplot(aes(x= Accuracy_Rate, y= reorder(Algoritma, -Accuracy_Rate))) +
  geom_line(stat="identity") +
  geom_point(aes(color=TT), size=3) +
  theme(legend.position="top") +
  theme(panel.background = element_rect(fill="white"))+
  xlab("Accuracy Rate") +
  ylab("Algoritma")

lgpred %>% 
  ggplot(aes(x= Sensivity, y= reorder(Algoritma, -Sensivity))) +
  geom_line(stat="identity") +
  geom_point(aes(color=TT), size=3) +
  theme(legend.position="top") +
  theme(panel.background = element_rect(fill="white"))+
  xlab("Sensitivity") +
  ylab("Algoritma")

lgpred %>% 
  ggplot(aes(x= Specificity, y= reorder(Algoritma, -Specificity))) +
  geom_line(stat="identity") +
  geom_point(aes(color=TT), size=3) +
  theme(legend.position="top") +
  theme(panel.background = element_rect(fill="white"))+
  xlab("Specificity") +
  ylab("Algoritma")


# Threshold'un 0.6 olarak seÃ§ildiÄŸi model en iyi model olarak seÃ§ilmiÅŸtir.

############################
### VarsayÄ±m Kontrolleri ###
############################


vif(logmodel1)
# herhangi bir baÄŸlantÄ± problemi gÃ¶rÃ¼lmemektedir.



##############################################################################################################################################

####################################
### Linear Discriminant Analysis ###
####################################

# Linear Discriminant Analiz yalnÄ±zca numerik deÄŸiÅŸkenler kullanÄ±labileceÄŸi iÃ§in numerik verilerden oluÅŸan bir veri seti oluÅŸturulup Ã¶yle devam edilecektir.

trainda <- train[,c(1,4,5,8,10,14)]
testda <- test[,c(1,4,5,8,10,14)]

pairs.panels(trainda[1:5],
             gap=0,
             bg=c("goldenrod1","firebrick4")[train$class],
             pch=21)

# pairplot grafiÄŸi incelendiÄŸinde:

# bazÄ± deÄŸiÅŸken Ã§iflerinin arasÄ±ndaki iliÅŸkide ayrÄ±ÅŸmanÄ±n hatalara raÄŸmen gerÃ§ekleÅŸtiÄŸi gÃ¶rÃ¼lmekte. bu deÄŸiÅŸken Ã§iftleri aÅŸaÄŸÄ±daki gibi sÄ±ralanabilir:
# Ã¶zellikle thalach deÄŸiÅŸkeninin diÄŸer deÄŸiÅŸkenlerle olan iliÅŸkisini inceleyen grafiklerde ayrÄ±ÅŸmalar daha bariz bir ÅŸekilde gerÃ§ekleÅŸmiÅŸ gibi gÃ¶rÃ¼nÃ¼yor.
# aynÄ± ÅŸekilde oldpeak deÄŸiÅŸkeninindeki ayrÄ±mlar da net bir ÅŸekilde gerÃ§ekleÅŸmiÅŸ gibi gÃ¶rÃ¼lmektedir.
# chol, trestbps ve age deÄŸiÅŸkenlerinde ayrÄ±mlarÄ±n ise daha az olduÄŸu gÃ¶rÃ¼lmektedir.
# deÄŸiÅŸkenlerin histogramlarÄ±na bakÄ±ldÄ±ÄŸÄ±nda Ã§arpÄ±klÄ±klara raÄŸmen normale yakÄ±n gÃ¶rÃ¼lmekteyken, Ã¶zellikle oldpeak deÄŸiÅŸkeninin histogramÄ±nda aÅŸÄ±rÄ± Ã§arpÄ±klÄ±k dikkat Ã§ekmektedir.
# korelasyonlar incelendiÄŸinde ise thalach ile age arasÄ±ndai -0.35'lik korelasyon en yÃ¼ksek korelasyon olarak dikkat Ã§ekmekte.
# oldpeak ile thalach arasÄ±ndaki korelasyon da -0.32 ile ikinci sÄ±rada yer almaktadÄ±r.

desc <- describeBy(trainda[1:5], trainda[,6]) # her bir tÃ¼r iÃ§in bakmak skor hesaplamasÄ± iÃ§in gerekli
desc

# sÄ±nÄ±flara gÃ¶re tanÄ±mlayÄ±cÄ± istatistikler incelendiÄŸinde:

# thalach deÄŸiÅŸkeninde sÄ±nÄ±flar arasÄ± ayrÄ±mÄ±n daha fazla olduÄŸu saptanmÄ±ÅŸtÄ±r.
# oldpeak deÄŸiÅŸkeni de thalach deÄŸiÅŸkeni kadar olmasa da ayrÄ±mÄ± saÄŸlamÄ±ÅŸ gibi gÃ¶rÃ¼nmekte.


model_lda<-lda(class~.,data=trainda)
model_lda

# Linear discriminant analysis modelinin Ã§Ä±ktÄ±sÄ± incelendiÄŸinde:

# YalnÄ±zca bir adet doÄŸrusal ayrÄ±m olduÄŸu saptanmÄ±ÅŸtÄ±r.
# Oranlar birbirine oldukÃ§a yakÄ±n Ã§Ä±kmÄ±ÅŸtÄ±r.


tahmin_1<-predict(model_lda,trainda)
hist_lda1<-ldahist(data=tahmin_1$x[,1],g=trainda$class) 

# Birinci fonksiyona gÃ¶re nasÄ±l ayrÄ±lmasÄ± gerektiÄŸini gÃ¶steren karÅŸÄ±laÅŸtÄ±rmalÄ± histogramlar incelendiÄŸinde: 
# YaklaÅŸÄ±k olarak -1 deÄŸeriyle 1 deÄŸerleri arasÄ±nda Ã¶rtÃ¼ÅŸme gÃ¶rÃ¼lmekte. 
# Bu olasÄ± yanlÄ±ÅŸ ayÄ±rmayÄ± gÃ¶steriyor olabilir.

# GÃ¶zlemlerin gruplara dahil olma olasÄ±lÄ±klarÄ±na bakÄ±ldÄ±ÄŸÄ±nda bazÄ± gÃ¶zlemlerin birbirine yakÄ±n olasÄ±lÄ±k deÄŸerleri olduÄŸu saptanmÄ±ÅŸtÄ±r. 
# Bu gÃ¶zlemlerden bazÄ±larÄ± aÅŸaÄŸÄ±daki gibi sÄ±ralanabilir.

tahmin_1$posterior[13,]
tahmin_1$posterior[14,]
tahmin_1$posterior[52,]


partimat(class~., data=trainda,method="lda") 

# Partition GrafiÄŸi incelendiÄŸinde: 

# GÃ¶zlemlerin kÄ±rmÄ±zÄ± renk ile gÃ¶sterilmesi yanlÄ±ÅŸ etiketlemeyi ifade etmektedir. 
# YanlÄ±ÅŸ etiketlenmenin en az olduÄŸu grafik oldpeak ile thalach arasÄ±ndaki iliÅŸkiyi ve ayrÄ±mÄ± gÃ¶steren grafikte gÃ¶rÃ¼lmekte.
# trestbps ile chol ve age ile trestbps arasÄ±ndaki grafikler incelendiÄŸinde ise ayrÄ±ÅŸmanÄ±n iyi bir ÅŸekilde yapÄ±lmadÄ±ÄŸÄ± dikkat Ã§ekmektedir.




### Tahminler

### Train

ldatrain <- predict(model_lda ,trainda)
caret::confusionMatrix(ldatrain$class, trainda$class)

# Accuracy Rate : 0.7488
# Sensitivity : 0.7963
# Specificity : 0.6970

### Test

ldatest <- predict(model_lda ,testda)
caret::confusionMatrix(ldatest$class, testda$class)

# Accuracy Rate : 0.7
# Sensitivity : 0.8269
# Specificity : 0.5263

preds[13,1] <- "lda"
preds[13,2] <- 0.7488
preds[13,3] <- 0.7
preds[13,4] <- 0.79
preds[13,5] <- 0.8269
preds[13,6] <- 0.6970
preds[13,7] <- 0.5263

##############################################################################################################################################

#######################################
### Quadratic Discriminant Analysis ###
#######################################

set.seed(2021900444)
train_indices <- sample(2, size=nrow(df), replace = TRUE, prob=c(0.7,0.3))
trainn <- df[train_indices==1, ]
qdatest <- df[train_indices==2, ]

trainn <- trainn[,c(1,4,5,8,10,14)]
qdatest <- qdatest[,c(1,4,5,8,10,14)]


model_qda <- qda(class~. , data=trainn) 

model_qda

# Quadratic Discriminant Analysis Ã§Ä±ktÄ±sÄ± incelendiÄŸinde:

# OlasÄ±lÄ±klar arasÄ±ndaki farkÄ±n bu denli dÃ¼ÅŸÃ¼k olmasÄ± modelin gÃ¼venilirliÄŸini sorgulatmaktadÄ±r.
# SÄ±fÄ±r ve birinci sÄ±nÄ±flarÄ±n deÄŸiÅŸken bazÄ±nda ortalamalarÄ±na bakÄ±ldÄ±ÄŸÄ± zaman trestbps, chol, thalach, oldpeak deÄŸiÅŸkenlerinin daha net bir ÅŸekilde ayrÄ±ÅŸtÄ±ÄŸÄ±nÄ±,
# diÄŸer deÄŸiÅŸkenler bazÄ±nda ise iyi bir ayrÄ±ÅŸma olmadÄ±ÄŸÄ± gÃ¶zlemlenmiÅŸtir. 

partimat(class~., data= trainn, method="qda")

# QDA iÃ§in Partition GrafiÄŸi incelendiÄŸinde: 

# GÃ¶zlemlerin kÄ±rmÄ±zÄ± renk ile gÃ¶sterilmesi yanlÄ±ÅŸ etiketlemeyi ifade etmektedir. 
# YanlÄ±ÅŸ etiketlenmenin en az olduÄŸu grafik oldpeak ile age ve oldpeak ile trestbps arasÄ±ndaki iliÅŸkiyi ve ayrÄ±mÄ± gÃ¶steren grafiklerde gÃ¶rÃ¼lmekte.
# trestbps ile chol ve age ile trestbps arasÄ±ndaki grafikler incelendiÄŸinde ise ayrÄ±ÅŸmanÄ±n iyi bir ÅŸekilde yapÄ±lmadÄ±ÄŸÄ± dikkat Ã§ekmektedir. LDA'da da bu aÃ§Ä±dan aynÄ± ÅŸekilde sonuÃ§lara eriÅŸilmiÅŸti.


### Tahminler

### Train

trainn <- predict(model_qda ,trainn)
caret::confusionMatrix(trainn$class, trainn$class)

# Accuracy Rate : 1
# Sensitivity : 1
# Specificity : 1

### Test

qdatest <- predict(model_qda ,qdatest)
caret::confusionMatrix(qdatest$class, qdatest$class)

# Accuracy Rate : 1
# Sensitivity : 1
# Specificity : 1

preds[14,1] <- "qda"
preds[14,2] <- 1.000001
preds[14,3] <- 1
preds[14,4] <- 1
preds[14,5] <- 1
preds[14,6] <- 1
preds[14,7] <- 1


############################
### VarsayÄ±m Kontrolleri ###
############################

### Multivariate Normallik Testleri 
dfmvn <- df[,c(1,4,5,8,10,14)]
sifir <- df[df$class==0,c(1,4,5,8,10,14)]
sifir <- sifir[,-6]

bir <- df[df$class==1, c(1,4,5,8,10,14)]
bir <- bir[,-6]

# Henze - Zirkler Testi

# Ho: Veri Ã§oklu normal daÄŸÄ±lÄ±mdan gelmektedir.
# Ha: Veri Ã§oklu normal daÄŸÄ±lÄ±mdan gelmemektedir. 


result <- mvn(data = dfmvn, subset = "class", mvnTest = "hz")
result$multivariateNormality

# Henze - Zinkler normallik testine gÃ¶re 0.05 anlamlÄ±lÄ±k dÃ¼zeyinde Ho verilerin Ã§oklu normal daÄŸÄ±lÄ±mdan geldiÄŸi hipotezi reddedilebilir.


# Mardia Testi 

# Ho: Veri Ã§oklu normal daÄŸÄ±lÄ±mdan gelmektedir.
# Ha: Veri Ã§oklu normal daÄŸÄ±lÄ±mdan gelmemektedir. 


resultmardia <- mvn(data = dfmvn, subset = "class", mvnTest = "mardia")
resultmardia$multivariateNormality

# Mardia Ã§ok deÄŸiÅŸkenli normallik testine gÃ¶re 0.05 anlamlÄ±lÄ±k dÃ¼zeyinde Ho verinin Ã§oklu normal daÄŸÄ±lÄ±mdan geldiÄŸi hipotezi reddedilebilir.

# Royston Testi

# Ho: Veri Ã§oklu normal daÄŸÄ±lÄ±mdan gelmektedir.
# Ha: Veri Ã§oklu normal daÄŸÄ±lÄ±mdan gelmemektedir.

resultroyston <- mvn(data = dfmvn, subset = "class", mvnTest = "royston")
resultroyston$multivariateNormality

# Royston Ã§ok deÄŸiÅŸkenli normallik testine gÃ¶re 0.05 anlamlÄ±lÄ±k dÃ¼zeyinde Ho verinin Ã§oklu normal daÄŸÄ±lÄ±mdan geldiÄŸi hipotezi reddedilebilir.


### Varyans HomojenliÄŸi Testi

### Levene Test

# Ho :	Ïƒ21=Ïƒ22=â€¦=Ïƒ2k
# Ha :	Ïƒ2iâ‰ Ïƒ2j    for at least one pair (i,j).

library(car)
leveneTest(df$age ~ as.factor(df$class), df) 

# Levene varyans homojenliÄŸi testine gÃ¶re 0.05 anlamlÄ±lÄ±k dÃ¼zeyinde Ho Age deÄŸiÅŸkeni iÃ§in sÄ±nÄ±flara gÃ¶re varyans homojenliÄŸi olduÄŸu hipotezini reddedilebilir. Yani Age deÄŸiÅŸkeninin varyansÄ± homojen deÄŸildir.

leveneTest(df$trestbps ~ as.factor(df$class), df) # homojen

# Levene varyans homojenliÄŸi testine gÃ¶re 0.05 anlamlÄ±lÄ±k dÃ¼zeyinde Ho trestbps deÄŸiÅŸkeni iÃ§in sÄ±nÄ±flara gÃ¶re varyans homojenliÄŸi olduÄŸu hipotezini reddedilemez. Yani trestbps deÄŸiÅŸkeninin varyansÄ± homojendir.

leveneTest(df$chol ~ as.factor(df$class), df)

# Levene varyans homojenliÄŸi testine gÃ¶re 0.05 anlamlÄ±lÄ±k dÃ¼zeyinde Ho chol deÄŸiÅŸkeni iÃ§in sÄ±nÄ±flara gÃ¶re varyans homojenliÄŸi olduÄŸu hipotezini reddedilemez. Yani chol deÄŸiÅŸkeninin varyansÄ± homojendir.


leveneTest(df$thalach ~ as.factor(df$class), df)


# Levene varyans homojenliÄŸi testine gÃ¶re 0.05 anlamlÄ±lÄ±k dÃ¼zeyinde Ho thalach deÄŸiÅŸkeni iÃ§in sÄ±nÄ±flara gÃ¶re varyans homojenliÄŸi olduÄŸu hipotezini reddedilebilir. Yani thalach deÄŸiÅŸkeninin varyansÄ± homojen deÄŸildir.


leveneTest(df$oldpeak ~ as.factor(df$class), df)

# Levene varyans homojenliÄŸi testine gÃ¶re 0.05 anlamlÄ±lÄ±k dÃ¼zeyinde Ho oldpeak deÄŸiÅŸkeni iÃ§in sÄ±nÄ±flara gÃ¶re varyans homojenliÄŸi olduÄŸu hipotezini reddedilebilir. Yani oldpeak deÄŸiÅŸkeninin varyansÄ± homojen deÄŸildir.



plota <- list()
box_variables <- c("trestbps", "age", "chol", "oldpeak", "thalach")
for(i in box_variables) {
  plota[[i]] <- ggplot(df, 
                      aes_string(x = "class", 
                                 y = i, 
                                 col = "class", 
                                 fill = "class")) + 
    geom_boxplot(alpha = 0.2) + 
    theme(legend.position = "none", panel.background = element_rect(fill = "white")) + 
    scale_color_manual(values = c("goldenrod1", "firebrick3")) +
    scale_fill_manual(values = c("goldenrod1", "firebrick3"))
}

grid.arrange(plota$trestbps, plota$age,plota$chol, plota$oldpeak, plota$thalach,  ncol=3)

# SayÄ±sal deÄŸiÅŸkenlerin baÄŸÄ±mlÄ± deÄŸiÅŸkenin sÄ±nÄ±flarÄ±na gÃ¶re grafikleri incelendiÄŸinde:

# trestbps ve chol deÄŸiÅŸkenlerin homojen varyanslÄ± olduÄŸu gÃ¶rÃ¼lmektedir.
# oldpeak, age ve thalach deÄŸiÅŸkenlerinin varyansÄ±nÄ±n homojen olmadÄ±ÄŸÄ± gÃ¶rÃ¼lmektedir.
# Bu sonuÃ§lar da levene test ile paralellik gÃ¶stermektedir.

### BoxM Testi

# Ho : Covariance matrices of the outcome variable are equal across all groups
# Ha : Covariance matrices of the outcome variable are different for at least one group


library(heplots)

boxm <- heplots::boxM(df[, c(1,4,5,8,10)], df$class) 
boxm 

# p-value 0.05'ten kÃ¼Ã§Ã¼k Ã§Ä±ktÄ±ÄŸÄ± iÃ§in 0.05 anlamlÄ±lÄ±k seviyesinde Ho baÄŸÄ±msÄ±z deÄŸiÅŸkenlerin kovaryans matrislerinin eÅŸit olduÄŸu varsayÄ±mÄ± reddedilebilir.

dev.off()
plot(boxm)


##############################################################################################################################################

###############################
### Support Vector Machines ###
###############################

#############################
# Support Vector Classifier #
#############################

# Direkt cross-validation ile model tune edilecektir.

set.seed(2021900444)
tune.out <- tune(svm ,
                 class~.,
                 data=train,
                 kernel ="linear",
                 ranges=list(cost=c(0.1,1,10,100,1000)))
summary(tune.out)

# 10 fold cross validation ile tune iÅŸlemi yaptÄ±ÄŸÄ±mÄ±zda:

# cost parametresi 1 olarak seÃ§ilmiÅŸtir. 
# cost parametresi ne kadar kÃ¼Ã§Ã¼k olursa yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rmalara o kadar az izin verileceÄŸi, dolayÄ±sÄ±yla marginin de o kadar dar olacaÄŸÄ± anlamÄ±na gelmektedir.
# bu model iÃ§in oldukÃ§a dÃ¼ÅŸÃ¼k ve dolayÄ±sÄ±yla dar bir margin olduÄŸu anlaÅŸÄ±lmÄ±ÅŸtÄ±r. 
# en dÃ¼ÅŸÃ¼k hata olarak ise 0.1788095 gÃ¶ze Ã§arpmaktadÄ±r.

linearsvmbest <- tune.out$best.model
summary(linearsvmbest)

### Linear Model Metrikleri

### Train

lineartrain <- predict(linearsvmbest ,train)

a <- caret::confusionMatrix(lineartrain, train$class)

# Accuracy Rate : 0.8792
# Sensitivity : 0.9259
# Specificity : 0.8283

### Test

lineartest <- predict(linearsvmbest ,test)

b <- caret::confusionMatrix(lineartest, test$class)

# Accuracy Rate : 0.8444
# Sensitivity : 0.9231
# Specificity : 0.7368

svmpreds <- data.frame()
svmpreds[1,1] <- "linear"
svmpreds[2,1] <- "linear"
svmpreds[1,2] <- "train"
svmpreds[2,2] <- "test"
svmpreds[1,3] <- a$overall[1]
svmpreds[2,3] <- b$overall[1]
svmpreds[1,4] <- a$byClass[1]
svmpreds[2,4] <- b$byClass[1]
svmpreds[1,5] <- a$byClass[2]
svmpreds[2,5] <- b$byClass[2]
############################################
### Polynomial Support Vector Classifier ###
############################################

tune.out <- tune(svm,
                 class~.,
                 data = train,
                 kernel = "polynomial",
                 ranges = list(cost = c(0.00001, 0.0001, 0.001, 0.01, 0.1),
                               degree = c(1, 3, 4, 5, 7)))

summary(tune.out)

# 10 fold cross validation ile tune iÅŸlemi yaptÄ±ÄŸÄ±mÄ±zda:

# cost parametresi 0.1 olarak seÃ§ilmiÅŸtir. 
# cost parametresi ne kadar kÃ¼Ã§Ã¼k olursa yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rmalara o kadar az izin verileceÄŸi, dolayÄ±sÄ±yla marginin de o kadar dar olacaÄŸÄ± anlamÄ±na gelmektedir.
# bu model iÃ§in oldukÃ§a dÃ¼ÅŸÃ¼k ve dolayÄ±sÄ±yla dar bir margin olduÄŸu anlaÅŸÄ±lmÄ±ÅŸtÄ±r. 
# degree ise 1 olarak saptanmÄ±ÅŸtÄ±r.
# en dÃ¼ÅŸÃ¼k hata olarak ise 0.47 gÃ¶ze Ã§arpmaktadÄ±r.

polysvmbest <- tune.out$best.model

### Polynomial Model Metrikleri

### Train

polytrain <- predict(polysvmbest ,train)

a <- caret::confusionMatrix(polytrain, train$class)

# Accuracy Rate : 0.8019
# Sensitivity : 0.8796
# Specificity : 0.7172

### Test

polytest <- predict(polysvmbest ,test)

b <- caret::confusionMatrix(polytest, test$class)

# Accuracy Rate : 0.7333
# Sensitivity : 0.8846
# Specificity : 0.5263

svmpreds[3,1] <- "poly"
svmpreds[4,1] <- "poly"
svmpreds[3,2] <- "train"
svmpreds[4,2] <- "test"
svmpreds[3,3] <- a$overall[1]
svmpreds[4,3] <- b$overall[1]
svmpreds[3,4] <- a$byClass[1]
svmpreds[4,4] <- b$byClass[1]
svmpreds[3,5] <- a$byClass[2]
svmpreds[4,5] <- b$byClass[2]


##########################
# Support Vector Machine #
##########################

# Direkt 10 fold cross validation ile en iyi modele karar verilecektir.

set.seed(2021900444)
tune.out <- tune(svm,
                     class~.,
                     data=train,
                     kernel ="radial",
                     ranges=list(cost=c(0.1,1,10,100,1000),
                                 gamma=c(0.1,1,3,4,0.5) ))
summary(tune.out)

# Radial kernel iÃ§in 10 fold cross validation ile tune iÅŸlemi yaptÄ±ÄŸÄ±mÄ±zda:

# cost parametresi 1 olarak seÃ§ilmiÅŸtir. 
# cost parametresi ne kadar kÃ¼Ã§Ã¼k olursa yanlÄ±ÅŸ sÄ±nÄ±flandÄ±rmalara o kadar az izin verileceÄŸi, dolayÄ±sÄ±yla marginin de o kadar dar olacaÄŸÄ± anlamÄ±na gelmektedir.
# bu model iÃ§in oldukÃ§a dÃ¼ÅŸÃ¼k ve dolayÄ±sÄ±yla dar bir margin olduÄŸu anlaÅŸÄ±lmÄ±ÅŸtÄ±r.
# gamma parametresi en dÃ¼ÅŸÃ¼k olan 0.1 seÃ§ilmiÅŸtir.
# bu oldukÃ§a dÃ¼ÅŸÃ¼k olan deÄŸer hiperdÃ¼zlemin esnekliÄŸinin oldukÃ§a dÃ¼ÅŸÃ¼k olduÄŸu anlamÄ±na gelmektedir
# en dÃ¼ÅŸÃ¼k hata olarak ise 0.21 olarak gÃ¶ze Ã§arpmaktadÄ±r.


radialsvmbest <- tune.out$best.model

### Radial Model Metrikleri

### Train

radialtrain <- predict(radialsvmbest ,train)

a <- caret::confusionMatrix(radialtrain, train$class)

# Accuracy Rate : 0.8792
# Sensitivity : 0.9259
# Specificity : 0.8283

### Test

radialtest <- predict(radialsvmbest ,test)

b <- caret::confusionMatrix(radialtest, test$class)

# Accuracy Rate : 0.8222
# Sensitivity : 0.9231
# Specificity : 0.6842




svmpreds[5,1] <- "radial"
svmpreds[6,1] <- "radial"
svmpreds[5,2] <- "train"
svmpreds[6,2] <- "test"
svmpreds[5,3] <- a$overall[1]
svmpreds[6,3] <- b$overall[1]
svmpreds[5,4] <- a$byClass[1]
svmpreds[6,4] <- b$byClass[1]
svmpreds[5,5] <- a$byClass[2]
svmpreds[6,5] <- b$byClass[2]


names(svmpreds) <- c("Algoritma", "TT", "Accuracy_Rate", "Sensivity", "Specificity" )



svmpreds %>% 
  ggplot(aes(x= Accuracy_Rate, y= reorder(Algoritma, -Accuracy_Rate))) +
  geom_line(stat="identity") +
  geom_point(aes(color=TT), size=3) +
  theme(legend.position="top") +
  theme(panel.background = element_rect(fill="white"))+
  xlab("Accuracy Rate") +
  ylab("Algoritma")



svmpreds %>% 
  ggplot(aes(x= Sensivity, y= reorder(Algoritma, -Sensivity))) +
  geom_line(stat="identity") +
  geom_point(aes(color=TT), size=3) +
  theme(legend.position="top") +
  theme(panel.background = element_rect(fill="white"))+
  xlab("Sensitivity") +
  ylab("Algoritma")


svmpreds %>% 
  ggplot(aes(x= Specificity, y= reorder(Algoritma, -Specificity))) +
  geom_line(stat="identity") +
  geom_point(aes(color=TT), size=3) +
  theme(legend.position="top") +
  theme(panel.background = element_rect(fill="white"))+
  xlab("Specificity") +
  ylab("Algoritma")



#################
### ROC Curve ###
#################

predct <- prediction(as.numeric(as.vector(prunedtree.predtest3)), test$class)
predbag <- prediction(as.numeric(as.vector(baggintest1)), test$class)
predrf <- prediction(as.numeric(as.vector(ranfortest1)), test$class)
predlr <- prediction(as.numeric(as.vector(testpred1)), test$class)
predlda <- prediction(as.numeric(as.vector(ldatest$class)), testda$class)
predqda <- prediction(as.numeric(as.vector(qdatest$class)), qdatest$class)
predsvm <- prediction(as.numeric(as.vector(radialtest)), test$class)

#####################
### AUC DeÄŸerleri ###
#####################

aucct <- performance( predct, measure= "auc" )
aucct <- aucct@y.values[[1]]

formatC(aucct, digits = 2)
aucbag <- performance(predbag, "auc")
aucbag <- aucbag@y.values[[1]]

aucrf <- performance(predrf, "auc")
aucrf <- aucrf@y.values[[1]]

auclr <- performance(predlr, "auc")
auclr <- auclr@y.values[[1]]

auclda <- performance(predlda, "auc")
auclda <- auclda@y.values[[1]]

aucqda <- performance(predqda, "auc")
aucqda <- aucqda@y.values[[1]]

aucsvm <- performance(predsvm, "auc")
aucsvm <- aucsvm@y.values[[1]]

#########################
### ROC EÄŸrileri Ä°Ã§in ###
#########################

perfct <- performance( predct, "tpr", "fpr" )
perfbag <- performance(predbag, "tpr", "fpr")
perfrf <- performance(predrf, "tpr", "fpr")
perflr <- performance(predlr, "tpr", "fpr")
perflda <- performance(predlda, "tpr", "fpr")
perfqda <- performance(predqda, "tpr", "fpr")
perfsvm <- performance(predsvm, "tpr", "fpr")

#################
### ROC Curve ###
#################

dev.off()
plot(perfct, col = "firebrick3", lwd = 2, lty = 1)
plot(perfbag, add = TRUE, col = "aquamarine3", lwd = 2, lty =2)
plot(perfrf, add=T, col = "chocolate3", lwd = 2, lty = 3)
plot(perflda, add = TRUE, col = "lightpink3", lwd = 2, lty =4)
plot(perfqda,add=T, col = "burlywood3", lwd = 2, lty =1)
plot(perflr, add = TRUE, col = "dodgerblue3", lwd = 2, lty=5)
plot(perfsvm, add=T, col = "darkorchid4", lwd = 2, lty=6)
legend("bottomright", title = "Algoritma - AUC DeÄŸeri", legend= c("ct - 0.77", "bag - 0.79", "rf - 0.75", "lda - 0.68", "qda - 1", "lr - 0.82", "svm - 0.8"), col = c( "firebrick3", "aquamarine3", "chocolate3","lightpink3","burlywood3","dodgerblue3","darkorchid4"), lty = c(1,2,3,4,1,5,6), lwd = 2)



#############################################
### Model Metriklerinin KarÅŸÄ±laÅŸtÄ±rÄ±lmasÄ± ###
#############################################

# Model metriklerini iÃ§eren bir veri seti oluÅŸturulmuÅŸtur.
# Bu veri setindeki deÄŸerler kullanÄ±larak ggplot ile accuracy rate, sensitivity, specificity deÄŸerleri iÃ§in ayrÄ± ayrÄ± grafikler Ã§izdirilecektir.

preds <- data.frame()
preds[1,1] <- "ClassTree"
preds[2,1] <- "ClassTree"
preds[1,2] <- "train"
preds[2,2] <- "test"
preds[1,3] <- 0.866 
preds[2,3] <- 0.8295 
preds[1,4] <- 0.9099
preds[2,4] <- 0.9184
preds[1,5] <- 0.8163
preds[2,5] <- 0.7179

##############################

preds[3,1] <- "Bagging"
preds[4,1] <- "Bagging"
preds[3,2] <- "train"
preds[4,2] <- "test"
preds[3,3] <- 1.0000
preds[4,3] <- 0.7955
preds[3,4] <- 1.0000
preds[4,4] <- 0.8776
preds[3,5] <- 1.0000
preds[4,5] <- 0.6923


#################################

preds[5,1] <- "RanFor"
preds[6,1] <- "RanFor"
preds[5,2] <- "train"
preds[6,2] <- "test"
preds[5,3] <- 0.9378
preds[6,3] <- 0.8295
preds[5,4] <- 0.9459
preds[6,4] <- 0.8776
preds[5,5] <- 0.9256
preds[6,5] <- 0.7692

#################################

preds[7,1] <- "LogReg"
preds[8,1] <- "LogReg"
preds[7,2] <- "train"
preds[8,2] <- "test"
preds[7,3] <- 0.8947
preds[8,3] <- 0.7955
preds[7,4] <- 0.9459
preds[8,4] <- 0.9184
preds[7,5] <- 0.8367
preds[8,5] <- 0.6410

#################################

preds[9,1] <- "LDA"
preds[10,1] <- "LDA"
preds[9,2] <- "train"
preds[10,2] <- "test"
preds[9,3] <- 0.7081
preds[10,3] <- 0.75
preds[9,4] <- 0.7838
preds[10,4] <- 0.7939
preds[9,5] <- 0.6224
preds[10,5] <- 0.6923

#################################

preds[11,1] <- "QDA"
preds[12,1] <- "QDA"
preds[11,2] <- "train"
preds[12,2] <- "test"
preds[11,3] <- 0.756
preds[12,3] <- 0.7955
preds[11,4] <- 0.86
preds[12,4] <- 0.8571
preds[11,5] <- 0.6837
preds[12,5] <- 0.6923

#################################

preds[13,1] <- "SVM"
preds[14,1] <- "SVM"
preds[13,2] <- "train"
preds[14,2] <- "test"
preds[13,3] <- 0.7943
preds[14,3] <- 0.7841
preds[13,4] <- 0.8919
preds[14,4] <- 0.8571
preds[13,5] <- 0.6837
preds[14,5] <- 0.6923

###############################

names(preds) <- c("Algoritma", "TT", "Accuracy_Rate", "Sensivity", "Specificity" )

preds

# Grafik Okuma Bilgisi:

# y ekseninde kullanÄ±lan algoritmalarÄ±n isimleri yer almaktadÄ±r.
# x ekseninde ise her bir algoritmanÄ±n accuracy rate deÄŸeri yer almaktadÄ±r.
# SarÄ± nokta o algoritmanÄ±n train veri seti ile kurulan modelinin accuracy rate'ini, kÄ±rmÄ±zÄ± nokta ise test veri seti ile kurulan modelin accuracy rate'ini gÃ¶stermektedir.
# Ä°ki nokta arasÄ±ndaki Ã§izginin uzunluÄŸu train ile test arasÄ±ndaki accuracy rate'inin ne kadar farklÄ± olduÄŸunu iÅŸaret etmektedir. bu da overfit ile ilgili bir fikir sahibi olabilmemize olanak saÄŸlayabilecektir.


#####################
### Accuracy Rate ###
#####################


preds %>% 
  ggplot(aes(x= Accuracy_Rate, y= reorder(Algoritma, -Accuracy_Rate))) +
  geom_line(stat="identity") +
  geom_point(aes(color=TT), size=4) +
  theme(legend.position="top") +
  theme(panel.background = element_rect(fill="white"))+
  xlab("Accuracy Rate") +
  ylab("Algoritma")

# Modellerin train ve test verilerine gÃ¶re Accuracy Rate'lerini gÃ¶steren grafik incelendiÄŸinde:

# LDA'nÄ±n train ile test accuracy rateleri arasÄ±nda Ã§ok fark olmadÄ±ÄŸÄ±, overfit problemi ile karÅŸÄ±laÅŸÄ±lmadÄ±ÄŸÄ± fark edilmiÅŸtir. Ancak diÄŸer algoritmalara kÄ±yasla daha dÃ¼ÅŸÃ¼k bir baÅŸarÄ± gÃ¶rÃ¼lmektedir. Zaten varsayÄ±mlar saÄŸlanmadÄ±ÄŸÄ± iÃ§in modelin gÃ¼venilirliÄŸi olmadÄ±ÄŸÄ± dÃ¼ÅŸÃ¼nÃ¼lmektedir.
# Random Forest'Ä±n train ile test accuracy rateleri arasÄ±ndaki fark tam sÄ±nÄ±rda gÃ¶rÃ¼lmekte. Ancak diÄŸer algoritmalara gÃ¶re daha dÃ¼ÅŸÃ¼k bir baÅŸarÄ± elde etmiÅŸ.
# Classification Tree'nin train ile test accuracy rateleri arasÄ±ndaki farkÄ±n Ã§ok fazla olmadÄ±ÄŸÄ± gÃ¶rÃ¼lmektedir. Yine diÄŸer yÃ¶ntemlere kÄ±yasla daha dÃ¼ÅŸÃ¼k bir baÅŸarÄ± fark ediliyor.
# Bagging'in train ile test accuracy rateleri arasÄ±ndaki fark incelendiÄŸinde overfit problemi olabileceÄŸi dÃ¼ÅŸÃ¼nÃ¼lmÃ¼ÅŸtÃ¼r. Accuracy rate yÃ¼ksek olmasÄ±na raÄŸmen overfit problemi modelin gÃ¼venilirliÄŸini sorgulatmaktadÄ±r.
# SVM'in train ile test accuracy rateleri arasÄ±ndaki fark incelendiÄŸinde overfit problemi gÃ¶rÃ¼lmemektedir. Accuracy Rateler de oldukÃ§a yÃ¼ksek gÃ¶rÃ¼lmektedir. BaÅŸarÄ±lÄ± olduÄŸu sÃ¶ylenebilir.
# Logistic Regression'Ä±n train ile test accuracy rateleri arasÄ±ndaki fark incelendiÄŸinde overfit problemi gÃ¶rÃ¼lmemektedir. Oran da oldukÃ§a yÃ¼ksek Ã§Ä±kmÄ±ÅŸtÄ±r. BaÅŸarÄ±lÄ± olduÄŸu sÃ¶ylenebilir.
# QDA'nÄ±n train ile test accuracy rateleri arasÄ±nda bir fark olmadÄ±ÄŸÄ± fark edilmiÅŸtir. Hem trainde hem de testte kusursuz baÅŸarÄ± saÄŸlamasÄ± oldukÃ§a enterasan. Ancak zaten varsayÄ±mlar saÄŸlanmadÄ±ÄŸÄ± iÃ§in bu modelin gÃ¼venililirliÄŸi yoktur.

###################
### Sensitivity ###
###################

preds %>% 
  ggplot(aes(x= Sensivity, y= reorder(Algoritma, -Sensivity))) +
  geom_line(stat="identity") +
  geom_point(aes(color=TT), size=3) +
  theme(legend.position="top") +
  theme(panel.background = element_rect(fill="white"))+
  xlab("Sensitivity") +
  ylab("Algoritma")

# Modellerin train ve test verilerine gÃ¶re Sensitivity'lerini gÃ¶steren grafik incelendiÄŸinde:

# HiÃ§bir algoritma iÃ§in bir overfit problemi gÃ¶rÃ¼lmemektedir.
# QDA'nÄ±n train ile test sensitivityleri arasÄ±nda bir fark olmadÄ±ÄŸÄ± fark edilmiÅŸtir. Hem trainde hem de testte kusursuz baÅŸarÄ± saÄŸlamasÄ± oldukÃ§a enterasan. Ancak zaten varsayÄ±mlar saÄŸlanmadÄ±ÄŸÄ± iÃ§in bu modelin gÃ¼venililirliÄŸi yoktur.
# Bir diÄŸer dikkat Ã§eken ayrÄ±ntÄ± SVM algoritmasÄ± ile oluÅŸturulmuÅŸ modelde gÃ¶rÃ¼lmektedir. Test veri setindeki sensitivity deÄŸeri train setindekine gÃ¶re daha yÃ¼ksek Ã§Ä±kmÄ±ÅŸtÄ±r. Bu tam olarak amaÃ§ladÄ±ÄŸÄ±mÄ±z bir ÅŸeydir. Modelin baÅŸarÄ±sÄ±nÄ± gÃ¼Ã§lendirmektedir.

# Sensitivity deÄŸeri bizim iÃ§in diÄŸer metriklere gÃ¶re daha Ã¶nemlidir. SÄ±nÄ±flandÄ±rmak istediÄŸimiz ayrÄ±m dÃ¼ÅŸÃ¼nÃ¼ldÃ¼ÄŸÃ¼nde sensitivity'nin Ã¶nemi ortaya Ã§Ä±kmaktadÄ±r.
# Kalp krizi geÃ§iren birini kalp krizi geÃ§irdi olarak sÄ±nÄ±flandÄ±rmak, kalp krizi geÃ§irmeyen birini kalp krizi geÃ§irmediÄŸi olarak sÄ±nÄ±flandÄ±rmaya kÄ±yasla Ã§ok daha Ã¶nemlidir.
# Ã‡Ã¼nkÃ¼ bu hayati Ã¶nem taÅŸÄ±yan bir ayrÄ±ntÄ± olduÄŸu iÃ§in model seÃ§imimizdeki ana faktÃ¶r olacak.

###################
### Specificity ###
###################

preds %>% 
  ggplot(aes(x= Specificity, y= reorder(Algoritma, -Specificity))) +
  geom_line(stat="identity") +
  geom_point(aes(color=TT), size=3) +
  theme(legend.position="top") +
  theme(panel.background = element_rect(fill="white"))+
  xlab("Specificity") +
  ylab("Algoritma")

# Modellerin train ve test verilerine gÃ¶re Sensitivity'lerini gÃ¶steren grafik incelendiÄŸinde:

# Bagging ve Random Forest iÃ§in bir overfit problemi gÃ¶rÃ¼lmektedir.
# QDA'nÄ±n train ile test specificityleri arasÄ±nda bir fark olmadÄ±ÄŸÄ± fark edilmiÅŸtir. Hem trainde hem de testte kusursuz baÅŸarÄ± saÄŸlamasÄ± oldukÃ§a enterasan. Ancak zaten varsayÄ±mlar saÄŸlanmadÄ±ÄŸÄ± iÃ§in bu modelin gÃ¼venililirliÄŸi yoktur.


######################
### En Uygun Model ###
######################

# ÃœÃ§ metrik iÃ§in train ve test verilerinde en yÃ¼ksek sonuÃ§ veren algoritmalar karÅŸÄ±laÅŸtÄ±rÄ±lmak istenilmiÅŸtir. 
# Ancak LDA ve QDA algoritmalarÄ±yla oluÅŸturulan modeller bu iki algoritmanÄ±n varsayÄ±mlarÄ±nÄ± saÄŸlamadÄ±ÄŸÄ± iÃ§in bu kÄ±yaslamadan Ã§Ä±karÄ±lmÄ±ÅŸtÄ±r.
# VarsayÄ±mlarÄ± saÄŸlamayan algoritmalar dikkate alÄ±nmayacaktÄ±r.


# Accuracy Rate:
# Train: Bagging, Random Forest
# Test: Logistic Regression, Support Vector Machine

# Sensitivity:
# Train: Bagging, Random Forest
# Test: SVM, Logistic Regression

# Specificity:
# Train: Bagging, Random Forest
# Test: Logistic Regression, Support Vector Machine

# AUC:
# Logictic Regression, Support Vector Machine

